{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dffde21-aa42-4f4d-b368-f006e8592bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from feature_eng.scalers import ranged_scaler\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4025533-4f77-4974-bd2a-a5e9742312e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as ptl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tensordict import TensorDict\n",
    "\n",
    "from castle.datasets import DAG, IIDSimulation \n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "\n",
    "import causica.distributions as cd\n",
    "\n",
    "from causica.functional_relationships import ICGNN\n",
    "from causica.training.auglag import AugLagLossCalculator, AugLagLR, AugLagLRConfig\n",
    "from causica.graph.dag_constraint import calculate_dagness\n",
    "\n",
    "from causica.datasets.variable_types import VariableTypeEnum\n",
    "from causica.datasets.tensordict_utils import tensordict_shapes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "COLORS = [\n",
    "    '#00B0F0',\n",
    "    '#FF0000',\n",
    "    '#B0F000'\n",
    "]\n",
    "\n",
    "# Set random seed\n",
    "SEED = 11\n",
    "np.random.seed(SEED)\n",
    "ptl.seed_everything(SEED)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819d1e8a-80bb-47f4-876b-77f648816b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlowh\\anaconda3\\envs\\py39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.graph.GraphNode import GraphNode\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c390f8eb-8264-459d-b5f2-1de365e6c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = pl.read_csv(\"data/data.csv\", separator=\",\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819b9f17-5bc3-49ee-b48d-a4e155ad96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pl.read_csv('data/metadata.csv',separator=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084c6abc-d959-409e-a4cc-693eb02edd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>aimp</th><th>amud</th><th>arnd</th><th>asin1</th><th>asin2</th><th>adbr</th><th>adfl</th><th>bed1</th><th>bed2</th><th>bfo1</th><th>bfo2</th><th>bso1</th><th>bso2</th><th>bso3</th><th>ced1</th><th>cfo1</th><th>cso1</th><th>y</th><th>category</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;2023-01-01 00:00:00&quot;</td><td>0.0</td><td>1.0</td><td>20.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:01&quot;</td><td>0.0</td><td>1.0</td><td>20.080031</td><td>0.00002</td><td>0.0002</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.9939e-7</td><td>0.000789</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000021</td><td>0.001229</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:02&quot;</td><td>0.0</td><td>1.0</td><td>20.276562</td><td>0.00004</td><td>0.0004</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000001</td><td>0.003115</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000104</td><td>0.004833</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:03&quot;</td><td>0.0</td><td>1.0</td><td>20.730938</td><td>0.00006</td><td>0.0006</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000003</td><td>0.006914</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000285</td><td>0.010688</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:04&quot;</td><td>0.0</td><td>1.0</td><td>21.118101</td><td>0.00008</td><td>0.0008</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000005</td><td>0.012123</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000601</td><td>0.018669</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 20)\n",
       "┌─────────────────────┬──────┬──────┬───────────┬───┬──────────┬──────────┬─────┬──────────┐\n",
       "│ timestamp           ┆ aimp ┆ amud ┆ arnd      ┆ … ┆ cfo1     ┆ cso1     ┆ y   ┆ category │\n",
       "│ ---                 ┆ ---  ┆ ---  ┆ ---       ┆   ┆ ---      ┆ ---      ┆ --- ┆ ---      │\n",
       "│ str                 ┆ f64  ┆ f64  ┆ f64       ┆   ┆ f64      ┆ f64      ┆ f64 ┆ f64      │\n",
       "╞═════════════════════╪══════╪══════╪═══════════╪═══╪══════════╪══════════╪═════╪══════════╡\n",
       "│ 2023-01-01 00:00:00 ┆ 0.0  ┆ 1.0  ┆ 20.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:01 ┆ 0.0  ┆ 1.0  ┆ 20.080031 ┆ … ┆ 0.000021 ┆ 0.001229 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:02 ┆ 0.0  ┆ 1.0  ┆ 20.276562 ┆ … ┆ 0.000104 ┆ 0.004833 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:03 ┆ 0.0  ┆ 1.0  ┆ 20.730938 ┆ … ┆ 0.000285 ┆ 0.010688 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:04 ┆ 0.0  ┆ 1.0  ┆ 21.118101 ┆ … ┆ 0.000601 ┆ 0.018669 ┆ 0.0 ┆ 0.0      │\n",
       "└─────────────────────┴──────┴──────┴───────────┴───┴──────────┴──────────┴─────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6eac40-7c24-4108-8c8c-76e3fa914456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>start_time</th><th>end_time</th><th>root_cause</th><th>affected</th><th>category</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;2023-01-12 15:11:45&quot;</td><td>&quot;2023-01-12 15:20:05&quot;</td><td>&quot;bso3&quot;</td><td>&quot;[&#x27;cfo1&#x27;]&quot;</td><td>12</td></tr><tr><td>&quot;2023-01-12 16:27:46&quot;</td><td>&quot;2023-01-12 17:51:06&quot;</td><td>&quot;bso3&quot;</td><td>&quot;[&#x27;cfo1&#x27;]&quot;</td><td>1</td></tr><tr><td>&quot;2023-01-12 18:19:35&quot;</td><td>&quot;2023-01-12 18:36:15&quot;</td><td>&quot;bfo2&quot;</td><td>&quot;[&#x27;cso1&#x27;]&quot;</td><td>8</td></tr><tr><td>&quot;2023-01-12 20:46:32&quot;</td><td>&quot;2023-01-12 20:51:32&quot;</td><td>&quot;bed2&quot;</td><td>&quot;[&#x27;ced1&#x27;]&quot;</td><td>7</td></tr><tr><td>&quot;2023-01-13 05:57:10&quot;</td><td>&quot;2023-01-13 06:02:10&quot;</td><td>&quot;bfo1&quot;</td><td>&quot;[&#x27;cfo1&#x27;]&quot;</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌─────────────────────┬─────────────────────┬────────────┬──────────┬──────────┐\n",
       "│ start_time          ┆ end_time            ┆ root_cause ┆ affected ┆ category │\n",
       "│ ---                 ┆ ---                 ┆ ---        ┆ ---      ┆ ---      │\n",
       "│ str                 ┆ str                 ┆ str        ┆ str      ┆ i64      │\n",
       "╞═════════════════════╪═════════════════════╪════════════╪══════════╪══════════╡\n",
       "│ 2023-01-12 15:11:45 ┆ 2023-01-12 15:20:05 ┆ bso3       ┆ ['cfo1'] ┆ 12       │\n",
       "│ 2023-01-12 16:27:46 ┆ 2023-01-12 17:51:06 ┆ bso3       ┆ ['cfo1'] ┆ 1        │\n",
       "│ 2023-01-12 18:19:35 ┆ 2023-01-12 18:36:15 ┆ bfo2       ┆ ['cso1'] ┆ 8        │\n",
       "│ 2023-01-12 20:46:32 ┆ 2023-01-12 20:51:32 ┆ bed2       ┆ ['ced1'] ┆ 7        │\n",
       "│ 2023-01-13 05:57:10 ┆ 2023-01-13 06:02:10 ┆ bfo1       ┆ ['cfo1'] ┆ 9        │\n",
       "└─────────────────────┴─────────────────────┴────────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf79826-0fcf-4c87-99fe-8132a81cac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cats_df.columns:\n",
    "    unique_vals = cats_df[col].n_unique()\n",
    "    data_type = cats_df[col].dtype\n",
    "    bad_dtypes = [pl.Date,pl.Datetime,pl.Utf8]\n",
    "    if ((unique_vals >= 50) & (data_type not in bad_dtypes) ):\n",
    "        cats_df = cats_df.with_columns(ranged_scaler(cats_df[col]))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e9b734-7d70-467b-904e-eb6831c30319",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = cats_df.with_columns( \n",
    "    pl.col(\"timestamp\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664248d7-e960-4bcc-a36f-feed0915603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>aimp</th><th>amud</th><th>arnd</th><th>asin1</th><th>asin2</th><th>adbr</th><th>adfl</th><th>bed1</th><th>bed2</th><th>bfo1</th><th>bfo2</th><th>bso1</th><th>bso2</th><th>bso3</th><th>ced1</th><th>cfo1</th><th>cso1</th><th>y</th><th>category</th></tr><tr><td>datetime[μs]</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2023-01-01 00:00:00</td><td>0.0</td><td>0.142857</td><td>-0.5</td><td>-4.1078e-14</td><td>2.0428e-14</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180547</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100389</td><td>-0.186623</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:01</td><td>0.0</td><td>0.142857</td><td>-0.495998</td><td>0.00002</td><td>0.0002</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.18054</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100389</td><td>-0.186618</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:02</td><td>0.0</td><td>0.142857</td><td>-0.486172</td><td>0.00004</td><td>0.0004</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180519</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.10039</td><td>-0.186604</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:03</td><td>0.0</td><td>0.142857</td><td>-0.463453</td><td>0.00006</td><td>0.0006</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180484</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100391</td><td>-0.18658</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:04</td><td>0.0</td><td>0.142857</td><td>-0.444095</td><td>0.00008</td><td>0.0008</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180437</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100393</td><td>-0.186548</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 20)\n",
       "┌─────────────────────┬──────┬──────────┬───────────┬───┬──────────┬───────────┬─────┬──────────┐\n",
       "│ timestamp           ┆ aimp ┆ amud     ┆ arnd      ┆ … ┆ cfo1     ┆ cso1      ┆ y   ┆ category │\n",
       "│ ---                 ┆ ---  ┆ ---      ┆ ---       ┆   ┆ ---      ┆ ---       ┆ --- ┆ ---      │\n",
       "│ datetime[μs]        ┆ f64  ┆ f64      ┆ f64       ┆   ┆ f64      ┆ f64       ┆ f64 ┆ f64      │\n",
       "╞═════════════════════╪══════╪══════════╪═══════════╪═══╪══════════╪═══════════╪═════╪══════════╡\n",
       "│ 2023-01-01 00:00:00 ┆ 0.0  ┆ 0.142857 ┆ -0.5      ┆ … ┆ 0.100389 ┆ -0.186623 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:01 ┆ 0.0  ┆ 0.142857 ┆ -0.495998 ┆ … ┆ 0.100389 ┆ -0.186618 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:02 ┆ 0.0  ┆ 0.142857 ┆ -0.486172 ┆ … ┆ 0.10039  ┆ -0.186604 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:03 ┆ 0.0  ┆ 0.142857 ┆ -0.463453 ┆ … ┆ 0.100391 ┆ -0.18658  ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:04 ┆ 0.0  ┆ 0.142857 ┆ -0.444095 ┆ … ┆ 0.100393 ┆ -0.186548 ┆ 0.0 ┆ 0.0      │\n",
       "└─────────────────────┴──────┴──────────┴───────────┴───┴──────────┴───────────┴─────┴──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c0b0bf-4445-4000-a06c-71828cdd5f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df['timestamp'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f9e5cd-7f3b-4264-ab71-93e8bc6dbfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_rows_list = metadata.rows(named=True)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "seed = 11\n",
    "ptl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b74e096-8866-4fe3-9256-5cc5ed5c760b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe0f8ab-f5d8-4b94-a24b-ac7af3bdd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    noise_dist=cd.ContinuousNoiseDist.SPLINE\n",
    "    batch_size=64\n",
    "    max_epoch=500\n",
    "    gumbel_temp=0.25\n",
    "    averaging_period=10\n",
    "    prior_sparsity_lambda=5.0\n",
    "    init_rho=1.0\n",
    "    init_alpha=0.0\n",
    "        \n",
    "training_config = TrainingConfig()\n",
    "auglag_config = AugLagLRConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad7a69-07d8-485a-b445-ce61072239e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%time\n",
    "new_metadata = []\n",
    "iteration = 0 \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "previous_fail = False\n",
    "for i, row in enumerate(cats_rows_list):\n",
    "    if previous_fail == True:\n",
    "        device =  'cpu'\n",
    "    else: \n",
    "        device = 'cuda:0'\n",
    "    try:\n",
    "        if i == 0:       \n",
    "            start_time = datetime.strptime(row['start_time'],'%Y-%m-%d %H:%M:%S')\n",
    "            end_time = datetime.strptime(row['end_time'],'%Y-%m-%d %H:%M:%S')\n",
    "            delta = end_time - start_time\n",
    "            start_time = start_time - delta\n",
    "        else:\n",
    "            start_time = end_time + timedelta(seconds=1)\n",
    "            end_time = datetime.strptime(row['end_time'],'%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "        \n",
    "        anomaly = eval(row['affected'])[0]\n",
    "        root_cause = row['root_cause']\n",
    "        \n",
    "        model_df = cats_df.filter( (pl.col('timestamp')>= start_time) & (pl.col('timestamp') <= end_time))\n",
    "        model_df = model_df.drop(['timestamp','y','category'])\n",
    "        out_cols = model_df.columns\n",
    "        cats_np = model_df.to_numpy()\n",
    "    \n",
    "        # Cast data to torch tensors\n",
    "        data_tensors = {}\n",
    "        \n",
    "        for i in range(cats_np.shape[1]):\n",
    "            data_tensors[out_cols[i]] = torch.tensor(cats_np[:, i].reshape(-1, 1))\n",
    "            \n",
    "        dataset_train = TensorDict(data_tensors, torch.Size([cats_np.shape[0]]))\n",
    "            \n",
    "        # Move the entire dataset to the device (for big datasets move to device by batch within training loop)\n",
    "        dataset_train = dataset_train.apply(lambda t: t.to(dtype=torch.float32, device=device)).to(device)\n",
    "        \n",
    "        \n",
    "        # Create loader\n",
    "        dataloader_train = DataLoader(\n",
    "            dataset=dataset_train,\n",
    "            collate_fn=lambda x: x,\n",
    "            batch_size=training_config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "    \n",
    "        num_nodes = len(dataset_train.keys())\n",
    "    \n",
    "        # Define the prior\n",
    "        prior = cd.GibbsDAGPrior(\n",
    "            num_nodes=num_nodes, \n",
    "            sparsity_lambda=training_config.prior_sparsity_lambda,\n",
    "           # expert_graph_container=expert_knowledge\n",
    "        )\n",
    "    \n",
    "            # Define the adjaceny module\n",
    "        adjacency_dist = cd.ENCOAdjacencyDistributionModule(num_nodes)\n",
    "        \n",
    "        #Define the functional module\n",
    "        icgnn = ICGNN(\n",
    "            variables=tensordict_shapes(dataset_train),\n",
    "            embedding_size=8, #32,\n",
    "            out_dim_g=8, #32,\n",
    "            norm_layer=torch.nn.LayerNorm,\n",
    "            res_connection=True,\n",
    "        )\n",
    "        \n",
    "        # Define the noise module\n",
    "        types_dict = {var_name: VariableTypeEnum.CONTINUOUS for var_name in dataset_train.keys()}\n",
    "        \n",
    "        noise_submodules = cd.create_noise_modules(\n",
    "            shapes=tensordict_shapes(dataset_train), \n",
    "            types=types_dict, \n",
    "            continuous_noise_dist=training_config.noise_dist\n",
    "        )\n",
    "        \n",
    "        noise_module = cd.JointNoiseModule(noise_submodules)\n",
    "    \n",
    "        sem_module = cd.SEMDistributionModule(\n",
    "        adjacency_module=adjacency_dist, \n",
    "        functional_relationships=icgnn, \n",
    "        noise_module=noise_module)\n",
    "    \n",
    "        sem_module.to(device)\n",
    "    \n",
    "        modules = {\n",
    "        \"icgnn\": sem_module.functional_relationships,\n",
    "        \"vardist\": sem_module.adjacency_module,\n",
    "        \"noise_dist\": sem_module.noise_module,\n",
    "        }\n",
    "        \n",
    "        parameter_list = [\n",
    "            {\"params\": module.parameters(), \"lr\": auglag_config.lr_init_dict[name], \"name\": name}\n",
    "            for name, module in modules.items()\n",
    "        ]\n",
    "        \n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(parameter_list)\n",
    "                \n",
    "        \n",
    "    \n",
    "        # Define the augmented Lagrangian loss objects\n",
    "        scheduler = AugLagLR(config=auglag_config)\n",
    "        \n",
    "        auglag_loss = AugLagLossCalculator(\n",
    "            init_alpha=training_config.init_alpha, \n",
    "            init_rho=training_config.init_rho\n",
    "        )\n",
    "    \n",
    "        assert len(dataset_train.batch_size) == 1, \"Only 1D batch size is supported\"\n",
    "    \n",
    "        num_samples = len(dataset_train)\n",
    "        \n",
    "        for epoch in range(training_config.max_epoch):\n",
    "            \n",
    "            for i, batch in enumerate(dataloader_train):\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Get SEM \n",
    "                sem_distribution = sem_module()\n",
    "                sem, *_ = sem_distribution.relaxed_sample(\n",
    "                    torch.Size([]), \n",
    "                    temperature=training_config.gumbel_temp\n",
    "                )  # soft sample\n",
    "                \n",
    "                # Compute the log probability of data\n",
    "                batch_log_prob = sem.log_prob(batch).mean()\n",
    "                \n",
    "                # Get the distribution entropy\n",
    "                sem_distribution_entropy = sem_distribution.entropy()\n",
    "                \n",
    "                # Compute the likelihood of the current graph\n",
    "                prior_term = prior.log_prob(sem.graph.to(device))\n",
    "                \n",
    "                # Compute the objective\n",
    "                objective = (-sem_distribution_entropy - prior_term) / num_samples - batch_log_prob\n",
    "                \n",
    "                # Compute the DAG-ness term\n",
    "                constraint = calculate_dagness(sem.graph)\n",
    "                \n",
    "                # Compute the Lagrangian loss\n",
    "                loss = auglag_loss(objective, constraint / num_samples)\n",
    "        \n",
    "                # Propagate gradients and update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update the Auglag parameters\n",
    "                scheduler.step(\n",
    "                    optimizer=optimizer,\n",
    "                    loss=auglag_loss,\n",
    "                    loss_value=loss.item(),\n",
    "                    lagrangian_penalty=constraint.item(),\n",
    "                )\n",
    "                \n",
    "                # Log metrics & plot the matrices\n",
    "                \"\"\"if epoch % 500 == 0 and i == 0:\n",
    "                    print(\n",
    "                        f\"epoch:{epoch} loss:{loss.item():.5g} nll:{-batch_log_prob.detach().cpu().numpy():.5g} \"\n",
    "                        f\"dagness:{constraint.item():.5f} num_edges:{(sem.graph > 0.0).sum()} \"\n",
    "                        f\"alpha:{auglag_loss.alpha:.5g} rho:{auglag_loss.rho:.5g} \"\n",
    "                        f\"step:{scheduler.outer_opt_counter}|{scheduler.step_counter} \"\n",
    "                        f\"num_lr_updates:{scheduler.num_lr_updates}\"\n",
    "                    )\"\"\"\n",
    "    \n",
    "        vardist = adjacency_dist()\n",
    "        pred_dag = vardist.mode.cpu().numpy()\n",
    "    \n",
    "        treatment_columns = set(out_cols)\n",
    "        treatment_columns.remove(anomaly)\n",
    "        treatment_columns = list(treatment_columns)\n",
    "    \n",
    "        estimated_ate = {}\n",
    "        num_samples = 1000\n",
    "        sample_shape = torch.Size([num_samples])\n",
    "        #normalizer = data_module.normalizer\n",
    "    \n",
    "        estimated_ate = {}\n",
    "        num_samples = 20000\n",
    "        sample_shape = torch.Size([num_samples])\n",
    "        #normalizer = data_module.normalizer\n",
    "        \n",
    "        for treatment in treatment_columns:\n",
    "            intervention_a = TensorDict({treatment: torch.tensor([1.0]).to(device)}, batch_size=tuple())\n",
    "            intervention_b = TensorDict({treatment: torch.tensor([0.0]).to(device)}, batch_size=tuple())\n",
    "        \n",
    "            rev_a_samples = (sem.do(interventions=intervention_a).sample(sample_shape))[anomaly]\n",
    "            rev_b_samples = (sem.do(interventions=intervention_b).sample(sample_shape))[anomaly]\n",
    "        \n",
    "            ate_mean = rev_a_samples.mean(0) - rev_b_samples.mean(0)\n",
    "            ate_std = np.sqrt((rev_a_samples.cpu().var(0) + rev_b_samples.cpu().var(0)) / num_samples)\n",
    "        \n",
    "            estimated_ate[treatment] = (\n",
    "                ate_mean.cpu().numpy()[0],\n",
    "                ate_std.cpu().numpy()[0],\n",
    "            )\n",
    "        \n",
    "        col_names = []\n",
    "        effects = []\n",
    "        for k, effect in estimated_ate.items():\n",
    "            col_names.append(k)\n",
    "            effects.append(np.abs(effect[0]))  \n",
    "    \n",
    "        top_causes = pd.DataFrame({\"variable\":col_names,'effect':effects}).sort_values(by='effect', ascending=False)[0:3]['variable'].reset_index(drop=True)\n",
    "    \n",
    "        if root_cause == top_causes[0]:\n",
    "            row['cause_1'] = 1\n",
    "        if root_cause == top_causes[1]:\n",
    "            row['cause_2'] = 1\n",
    "        if root_cause == top_causes[2]:\n",
    "            row['cause_3'] = 1\n",
    "        new_metadata.append(row)\n",
    "        if iteration%50 == 0:\n",
    "            print(iteration)\n",
    "        iteration+=1\n",
    "\n",
    "\n",
    "        del sem\n",
    "        del intervention_a\n",
    "        del intervention_b\n",
    "        del dataset_train\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        previous_fail = True\n",
    "        print(e)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90267640-601a-41fd-b39c-598b8342bcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20689655172413793"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpge_stats = pl.DataFrame(new_metadata)\n",
    "agg_stats = mpge_stats.select(pl.sum(\"cause_1\", \"cause_2\",'cause_3'))\n",
    "agg_stats.select(pl.sum_horizontal(pl.all())).item()/mpge_stats.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f840e0c2-93a4-452c-a6f2-464fbced4f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpge_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80572339-9399-4cec-bcc4-3755f55ee735",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_ate = {}\n",
    "num_samples = 20000\n",
    "sample_shape = torch.Size([num_samples])\n",
    "\n",
    "\n",
    "#normalizer = data_module.normalizer\n",
    "\n",
    "for treatment in treatment_columns:\n",
    "    intervention_a = TensorDict({treatment: torch.tensor([1.0]).to(device)}, batch_size=tuple())\n",
    "    intervention_b = TensorDict({treatment: torch.tensor([0.0]).to(device)}, batch_size=tuple()) \n",
    "    rev_b_samples = (sem.do(interventions=intervention_b).sample(sample_shape))[anomaly]\n",
    "\n",
    "    ate_mean = rev_a_samples.mean(0) - rev_b_samples.mean(0)\n",
    "    ate_std = np.sqrt((rev_a_samples.cpu().var(0) + rev_b_samples.cpu().var(0)) / num_samples)\n",
    "\n",
    "    estimated_ate[treatment] = (\n",
    "        ate_mean.cpu().numpy()[0],\n",
    "        ate_std.cpu().numpy()[0],\n",
    "    )\n",
    "estimated_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ddb75-5b68-4289-954c-f91ff136a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "effects = []\n",
    "for k, effect in estimated_ate.items():\n",
    "    col_names.append(k)\n",
    "    effects.append(np.abs(effect[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9a27b-fa7f-40fa-95cf-407d454d5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_causes = pd.DataFrame({\"variable\":col_names,'effect':effects}).sort_values(by='effect', ascending=False)[0:3]['variable'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18229fb5-b4cf-4345-b410-c3f87e8f3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_causes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cfdcf-0f73-44b9-8042-7b87e8be983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top_causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc1451-920d-446d-b1ab-cf552649b4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
