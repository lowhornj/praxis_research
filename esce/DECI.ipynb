{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dffde21-aa42-4f4d-b368-f006e8592bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from feature_eng.scalers import ranged_scaler\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4025533-4f77-4974-bd2a-a5e9742312e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as ptl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tensordict import TensorDict\n",
    "\n",
    "from castle.datasets import DAG, IIDSimulation \n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "\n",
    "import causica.distributions as cd\n",
    "\n",
    "from causica.functional_relationships import ICGNN\n",
    "from causica.training.auglag import AugLagLossCalculator, AugLagLR, AugLagLRConfig\n",
    "from causica.graph.dag_constraint import calculate_dagness\n",
    "\n",
    "from causica.datasets.variable_types import VariableTypeEnum\n",
    "from causica.datasets.tensordict_utils import tensordict_shapes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "COLORS = [\n",
    "    '#00B0F0',\n",
    "    '#FF0000',\n",
    "    '#B0F000'\n",
    "]\n",
    "\n",
    "# Set random seed\n",
    "SEED = 11\n",
    "np.random.seed(SEED)\n",
    "ptl.seed_everything(SEED)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819d1e8a-80bb-47f4-876b-77f648816b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlowh\\anaconda3\\envs\\py39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.graph.GraphNode import GraphNode\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c390f8eb-8264-459d-b5f2-1de365e6c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = pl.read_csv(\"data/data.csv\", separator=\",\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819b9f17-5bc3-49ee-b48d-a4e155ad96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pl.read_csv('data/metadata.csv',separator=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084c6abc-d959-409e-a4cc-693eb02edd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>aimp</th><th>amud</th><th>arnd</th><th>asin1</th><th>asin2</th><th>adbr</th><th>adfl</th><th>bed1</th><th>bed2</th><th>bfo1</th><th>bfo2</th><th>bso1</th><th>bso2</th><th>bso3</th><th>ced1</th><th>cfo1</th><th>cso1</th><th>y</th><th>category</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;2023-01-01 00:00:00&quot;</td><td>0.0</td><td>1.0</td><td>20.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:01&quot;</td><td>0.0</td><td>1.0</td><td>20.080031</td><td>0.00002</td><td>0.0002</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.9939e-7</td><td>0.000789</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000021</td><td>0.001229</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:02&quot;</td><td>0.0</td><td>1.0</td><td>20.276562</td><td>0.00004</td><td>0.0004</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000001</td><td>0.003115</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000104</td><td>0.004833</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:03&quot;</td><td>0.0</td><td>1.0</td><td>20.730938</td><td>0.00006</td><td>0.0006</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000003</td><td>0.006914</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000285</td><td>0.010688</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;2023-01-01 00:00:04&quot;</td><td>0.0</td><td>1.0</td><td>21.118101</td><td>0.00008</td><td>0.0008</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000005</td><td>0.012123</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.000601</td><td>0.018669</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 20)\n",
       "┌─────────────────────┬──────┬──────┬───────────┬───┬──────────┬──────────┬─────┬──────────┐\n",
       "│ timestamp           ┆ aimp ┆ amud ┆ arnd      ┆ … ┆ cfo1     ┆ cso1     ┆ y   ┆ category │\n",
       "│ ---                 ┆ ---  ┆ ---  ┆ ---       ┆   ┆ ---      ┆ ---      ┆ --- ┆ ---      │\n",
       "│ str                 ┆ f64  ┆ f64  ┆ f64       ┆   ┆ f64      ┆ f64      ┆ f64 ┆ f64      │\n",
       "╞═════════════════════╪══════╪══════╪═══════════╪═══╪══════════╪══════════╪═════╪══════════╡\n",
       "│ 2023-01-01 00:00:00 ┆ 0.0  ┆ 1.0  ┆ 20.0      ┆ … ┆ 0.0      ┆ 0.0      ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:01 ┆ 0.0  ┆ 1.0  ┆ 20.080031 ┆ … ┆ 0.000021 ┆ 0.001229 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:02 ┆ 0.0  ┆ 1.0  ┆ 20.276562 ┆ … ┆ 0.000104 ┆ 0.004833 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:03 ┆ 0.0  ┆ 1.0  ┆ 20.730938 ┆ … ┆ 0.000285 ┆ 0.010688 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:04 ┆ 0.0  ┆ 1.0  ┆ 21.118101 ┆ … ┆ 0.000601 ┆ 0.018669 ┆ 0.0 ┆ 0.0      │\n",
       "└─────────────────────┴──────┴──────┴───────────┴───┴──────────┴──────────┴─────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6eac40-7c24-4108-8c8c-76e3fa914456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>start_time</th><th>end_time</th><th>root_cause</th><th>affected</th><th>category</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;2023-01-12 15:11:45&quot;</td><td>&quot;2023-01-12 15:20:05&quot;</td><td>&quot;bso3&quot;</td><td>&quot;[&#x27;cfo1&#x27;]&quot;</td><td>12</td></tr><tr><td>&quot;2023-01-12 16:27:46&quot;</td><td>&quot;2023-01-12 17:51:06&quot;</td><td>&quot;bso3&quot;</td><td>&quot;[&#x27;cfo1&#x27;]&quot;</td><td>1</td></tr><tr><td>&quot;2023-01-12 18:19:35&quot;</td><td>&quot;2023-01-12 18:36:15&quot;</td><td>&quot;bfo2&quot;</td><td>&quot;[&#x27;cso1&#x27;]&quot;</td><td>8</td></tr><tr><td>&quot;2023-01-12 20:46:32&quot;</td><td>&quot;2023-01-12 20:51:32&quot;</td><td>&quot;bed2&quot;</td><td>&quot;[&#x27;ced1&#x27;]&quot;</td><td>7</td></tr><tr><td>&quot;2023-01-13 05:57:10&quot;</td><td>&quot;2023-01-13 06:02:10&quot;</td><td>&quot;bfo1&quot;</td><td>&quot;[&#x27;cfo1&#x27;]&quot;</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌─────────────────────┬─────────────────────┬────────────┬──────────┬──────────┐\n",
       "│ start_time          ┆ end_time            ┆ root_cause ┆ affected ┆ category │\n",
       "│ ---                 ┆ ---                 ┆ ---        ┆ ---      ┆ ---      │\n",
       "│ str                 ┆ str                 ┆ str        ┆ str      ┆ i64      │\n",
       "╞═════════════════════╪═════════════════════╪════════════╪══════════╪══════════╡\n",
       "│ 2023-01-12 15:11:45 ┆ 2023-01-12 15:20:05 ┆ bso3       ┆ ['cfo1'] ┆ 12       │\n",
       "│ 2023-01-12 16:27:46 ┆ 2023-01-12 17:51:06 ┆ bso3       ┆ ['cfo1'] ┆ 1        │\n",
       "│ 2023-01-12 18:19:35 ┆ 2023-01-12 18:36:15 ┆ bfo2       ┆ ['cso1'] ┆ 8        │\n",
       "│ 2023-01-12 20:46:32 ┆ 2023-01-12 20:51:32 ┆ bed2       ┆ ['ced1'] ┆ 7        │\n",
       "│ 2023-01-13 05:57:10 ┆ 2023-01-13 06:02:10 ┆ bfo1       ┆ ['cfo1'] ┆ 9        │\n",
       "└─────────────────────┴─────────────────────┴────────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf79826-0fcf-4c87-99fe-8132a81cac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cats_df.columns:\n",
    "    unique_vals = cats_df[col].n_unique()\n",
    "    data_type = cats_df[col].dtype\n",
    "    bad_dtypes = [pl.Date,pl.Datetime,pl.Utf8]\n",
    "    if ((unique_vals >= 50) & (data_type not in bad_dtypes) ):\n",
    "        cats_df = cats_df.with_columns(ranged_scaler(cats_df[col]))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e9b734-7d70-467b-904e-eb6831c30319",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = cats_df.with_columns( \n",
    "    pl.col(\"timestamp\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664248d7-e960-4bcc-a36f-feed0915603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>aimp</th><th>amud</th><th>arnd</th><th>asin1</th><th>asin2</th><th>adbr</th><th>adfl</th><th>bed1</th><th>bed2</th><th>bfo1</th><th>bfo2</th><th>bso1</th><th>bso2</th><th>bso3</th><th>ced1</th><th>cfo1</th><th>cso1</th><th>y</th><th>category</th></tr><tr><td>datetime[μs]</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2023-01-01 00:00:00</td><td>0.0</td><td>0.142857</td><td>-0.5</td><td>-4.1078e-14</td><td>2.0428e-14</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180547</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100389</td><td>-0.186623</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:01</td><td>0.0</td><td>0.142857</td><td>-0.495998</td><td>0.00002</td><td>0.0002</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.18054</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100389</td><td>-0.186618</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:02</td><td>0.0</td><td>0.142857</td><td>-0.486172</td><td>0.00004</td><td>0.0004</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180519</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.10039</td><td>-0.186604</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:03</td><td>0.0</td><td>0.142857</td><td>-0.463453</td><td>0.00006</td><td>0.0006</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180484</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100391</td><td>-0.18658</td><td>0.0</td><td>0.0</td></tr><tr><td>2023-01-01 00:00:04</td><td>0.0</td><td>0.142857</td><td>-0.444095</td><td>0.00008</td><td>0.0008</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180437</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100393</td><td>-0.186548</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 20)\n",
       "┌─────────────────────┬──────┬──────────┬───────────┬───┬──────────┬───────────┬─────┬──────────┐\n",
       "│ timestamp           ┆ aimp ┆ amud     ┆ arnd      ┆ … ┆ cfo1     ┆ cso1      ┆ y   ┆ category │\n",
       "│ ---                 ┆ ---  ┆ ---      ┆ ---       ┆   ┆ ---      ┆ ---       ┆ --- ┆ ---      │\n",
       "│ datetime[μs]        ┆ f64  ┆ f64      ┆ f64       ┆   ┆ f64      ┆ f64       ┆ f64 ┆ f64      │\n",
       "╞═════════════════════╪══════╪══════════╪═══════════╪═══╪══════════╪═══════════╪═════╪══════════╡\n",
       "│ 2023-01-01 00:00:00 ┆ 0.0  ┆ 0.142857 ┆ -0.5      ┆ … ┆ 0.100389 ┆ -0.186623 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:01 ┆ 0.0  ┆ 0.142857 ┆ -0.495998 ┆ … ┆ 0.100389 ┆ -0.186618 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:02 ┆ 0.0  ┆ 0.142857 ┆ -0.486172 ┆ … ┆ 0.10039  ┆ -0.186604 ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:03 ┆ 0.0  ┆ 0.142857 ┆ -0.463453 ┆ … ┆ 0.100391 ┆ -0.18658  ┆ 0.0 ┆ 0.0      │\n",
       "│ 2023-01-01 00:00:04 ┆ 0.0  ┆ 0.142857 ┆ -0.444095 ┆ … ┆ 0.100393 ┆ -0.186548 ┆ 0.0 ┆ 0.0      │\n",
       "└─────────────────────┴──────┴──────────┴───────────┴───┴──────────┴───────────┴─────┴──────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c0b0bf-4445-4000-a06c-71828cdd5f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df['timestamp'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f9e5cd-7f3b-4264-ab71-93e8bc6dbfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_rows_list = metadata.rows(named=True)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "seed = 11\n",
    "ptl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b74e096-8866-4fe3-9256-5cc5ed5c760b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe0f8ab-f5d8-4b94-a24b-ac7af3bdd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    noise_dist=cd.ContinuousNoiseDist.SPLINE\n",
    "    batch_size=64\n",
    "    max_epoch=500\n",
    "    gumbel_temp=0.25\n",
    "    averaging_period=10\n",
    "    prior_sparsity_lambda=5.0\n",
    "    init_rho=1.0\n",
    "    init_alpha=0.0\n",
    "        \n",
    "training_config = TrainingConfig()\n",
    "auglag_config = AugLagLRConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c74bc6-a323-41ef-9eda-e27f7c434a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating alpha to: 3.7537364959716797\n",
      "Updating alpha to: 5.414556503295898\n",
      "Updating alpha to: 27.072782516479492\n",
      "0\n",
      "Updating alpha to: 41.005699157714844\n",
      "Updating alpha to: 205.02849578857422\n",
      "Updating alpha to: 1025.142478942871\n",
      "Updating alpha to: 5125.7123947143555\n",
      "Updating alpha to: 25628.561973571777\n",
      "Updating alpha to: 128142.80986785889\n",
      "Updating alpha to: 14.789104461669922\n",
      "Updating alpha to: 18.26199722290039\n",
      "Updating alpha to: 19.448284149169922\n",
      "Updating rho, dag penalty prev:  1.1862869263\n",
      "Updating rho, dag penalty prev:  1.1862869263\n",
      "Updating rho, dag penalty prev:  1.1862869263\n",
      "Updating rho, dag penalty prev:  1.1862869263\n",
      "Updating rho, dag penalty prev:  1.1862869263\n",
      "Updating rho, dag penalty prev:  1.1862869263\n",
      "Updating rho, dag penalty prev:  1.1862869263\n",
      "Updating alpha to: 39.206478118896484\n",
      "Updating alpha to: 45.216800689697266\n",
      "Updating rho, dag penalty prev:  6.0103225708\n",
      "Updating rho, dag penalty prev:  6.0103225708\n",
      "Updating rho, dag penalty prev:  6.0103225708\n",
      "Updating rho, dag penalty prev:  6.0103225708\n",
      "Updating rho, dag penalty prev:  6.0103225708\n",
      "Updating rho, dag penalty prev:  6.0103225708\n",
      "Updating alpha to: 3514361.7756385803\n",
      "Updating alpha to: 17571808.8781929\n",
      "Updating alpha to: 87859044.39096451\n",
      "Updating alpha to: 439295221.95482254\n",
      "Updating alpha to: 2196476109.7741127\n",
      "Updating alpha to: 10982380548.870564\n",
      "Updating alpha to: 139.46385192871094\n",
      "Updating alpha to: 150.86223602294922\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating rho, dag penalty prev:  11.3983840942\n",
      "Updating alpha to: 593876798.8114548\n",
      "Updating alpha to: 665299184.9808884\n",
      "Updating alpha to: 3326495924.904442\n",
      "Updating alpha to: 16632479624.52221\n",
      "Updating alpha to: 83162398122.61105\n",
      "Updating alpha to: 415811990613.0553\n",
      "Updating alpha to: 2079059953065.2764\n",
      "Expected value argument (Tensor of shape (64, 1)) to be within the support (Real()) of the distribution Normal(loc: torch.Size([64, 1]), scale: torch.Size([64, 1])), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Updating alpha to: 19.922622680664062\n",
      "Updating alpha to: 25.948034286499023\n",
      "Updating rho, dag penalty prev:  6.0254116058\n",
      "Updating rho, dag penalty prev:  6.0254116058\n",
      "Updating rho, dag penalty prev:  6.0254116058\n",
      "Updating rho, dag penalty prev:  6.0254116058\n",
      "Updating rho, dag penalty prev:  6.0254116058\n",
      "Updating alpha to: 364003.38028526306\n",
      "Updating alpha to: 1820016.9014263153\n",
      "Updating alpha to: 9100084.507131577\n",
      "Updating alpha to: 45500422.53565788\n",
      "Updating alpha to: 227502112.6782894\n",
      "Updating alpha to: 1137510563.391447\n",
      "Updating alpha to: 77.9710464477539\n",
      "Updating alpha to: 87.47838592529297\n",
      "Updating rho, dag penalty prev:  9.5073394775\n",
      "Updating rho, dag penalty prev:  9.5073394775\n",
      "Updating rho, dag penalty prev:  9.5073394775\n",
      "Updating rho, dag penalty prev:  9.5073394775\n",
      "Updating rho, dag penalty prev:  9.5073394775\n",
      "Updating rho, dag penalty prev:  9.5073394775\n",
      "Updating rho, dag penalty prev:  9.5073394775\n",
      "Updating alpha to: 51592799.880729675\n",
      "Updating alpha to: 61760131.576286316\n",
      "Updating alpha to: 66801864.36437225\n",
      "Updating alpha to: 334009321.82186127\n",
      "Updating alpha to: 1670046609.1093063\n",
      "Updating alpha to: 8350233045.546532\n",
      "Updating alpha to: 41751165227.73266\n",
      "Updating alpha to: 208755826138.6633\n",
      "Updating alpha to: 47.173133850097656\n",
      "Updating alpha to: 50.19174003601074\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating rho, dag penalty prev:  3.0186061859\n",
      "Updating alpha to: 250.9587001800537\n",
      "Updating alpha to: 1254.7935009002686\n",
      "Updating alpha to: 6273.967504501343\n",
      "Updating alpha to: 31369.837522506714\n",
      "Updating alpha to: 156849.18761253357\n",
      "Updating alpha to: 65.24850463867188\n",
      "Updating alpha to: 73.19949722290039\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating rho, dag penalty prev:  7.9509925842\n",
      "Updating alpha to: 152090718.035923\n",
      "Updating alpha to: 202508045.91678238\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 1012540229.5839119\n",
      "Updating alpha to: 5062701147.9195595\n",
      "Updating alpha to: 25313505739.597797\n",
      "Updating alpha to: 126567528697.98898\n",
      "Updating alpha to: 632837643489.945\n",
      "Updating alpha to: 98.12479400634766\n",
      "Updating alpha to: 110.57979583740234\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating rho, dag penalty prev:  12.4550018311\n",
      "Updating alpha to: 569549480.3918076\n",
      "Updating alpha to: 636782566.0241318\n",
      "Updating rho, dag penalty prev:  0.6723308563\n",
      "Updating alpha to: 3183912830.120659\n",
      "Updating alpha to: 15919564150.603294\n",
      "Updating alpha to: 79597820753.01648\n",
      "Updating alpha to: 397989103765.0824\n",
      "Updating alpha to: 1989945518825.412\n",
      "Updating alpha to: 9.085166931152344\n",
      "Updating alpha to: 10.17829704284668\n",
      "Updating rho, dag penalty prev:  1.0931301117\n",
      "Updating alpha to: 68.04150390625\n",
      "Updating alpha to: 81.37076187133789\n",
      "Updating rho, dag penalty prev:  13.3292579651\n",
      "Updating rho, dag penalty prev:  13.3292579651\n",
      "Updating rho, dag penalty prev:  13.3292579651\n",
      "Updating rho, dag penalty prev:  13.3292579651\n",
      "Updating rho, dag penalty prev:  13.3292579651\n",
      "Updating rho, dag penalty prev:  13.3292579651\n",
      "Updating rho, dag penalty prev:  13.3292579651\n",
      "Updating alpha to: 72390790.29410172\n",
      "Updating alpha to: 361953951.4705086\n",
      "Updating alpha to: 1809769757.3525429\n",
      "Updating alpha to: 9048848786.762714\n",
      "Updating alpha to: 45244243933.81357\n",
      "Updating alpha to: 226221219669.06784\n",
      "Updating alpha to: 36.019989013671875\n",
      "Updating alpha to: 37.92232131958008\n",
      "Updating rho, dag penalty prev:  1.9023323059\n",
      "Updating rho, dag penalty prev:  1.9023323059\n",
      "Updating rho, dag penalty prev:  1.9023323059\n",
      "Updating rho, dag penalty prev:  1.9023323059\n",
      "Updating alpha to: 11066.822467803955\n",
      "Updating alpha to: 12817.844806671143\n",
      "Updating rho, dag penalty prev:  0.1751022339\n",
      "Updating rho, dag penalty prev:  0.1751022339\n",
      "Updating rho, dag penalty prev:  0.1751022339\n",
      "Updating rho, dag penalty prev:  0.1751022339\n",
      "Updating rho, dag penalty prev:  0.1751022339\n",
      "Updating alpha to: 64089.22403335571\n",
      "Updating alpha to: 320446.12016677856\n",
      "Updating alpha to: 1602230.6008338928\n",
      "Updating alpha to: 8011153.004169464\n",
      "Updating alpha to: 40055765.02084732\n",
      "index 8 is out of bounds for dimension 1 with size 8\n",
      "Updating alpha to: 56.878929138183594\n",
      "Updating alpha to: 64.63838768005371\n",
      "Updating rho, dag penalty prev:  7.7594585419\n",
      "Updating rho, dag penalty prev:  7.7594585419\n",
      "Updating rho, dag penalty prev:  7.7594585419\n",
      "Updating rho, dag penalty prev:  7.7594585419\n",
      "Updating rho, dag penalty prev:  7.7594585419\n",
      "Updating rho, dag penalty prev:  7.7594585419\n",
      "Updating alpha to: 3350183.275472641\n",
      "Updating alpha to: 16750916.377363205\n",
      "Updating alpha to: 83754581.88681602\n",
      "Updating alpha to: 418772909.4340801\n",
      "Updating alpha to: 34.07575607299805\n",
      "Updating alpha to: 35.26204299926758\n",
      "Updating alpha to: 35.76621627807617\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 68.36869812011719\n",
      "Updating alpha to: 73.6158561706543\n",
      "Updating rho, dag penalty prev:  5.2471580505\n",
      "Updating rho, dag penalty prev:  5.2471580505\n",
      "Updating alpha to: 388.42947006225586\n",
      "Updating rho, dag penalty prev:  3.1481361389\n",
      "Updating rho, dag penalty prev:  3.1481361389\n",
      "Updating rho, dag penalty prev:  3.1481361389\n",
      "Updating rho, dag penalty prev:  3.1481361389\n",
      "Updating rho, dag penalty prev:  3.1481361389\n",
      "Updating alpha to: 20068328.187770844\n",
      "Updating alpha to: 100341640.93885422\n",
      "Updating alpha to: 501708204.6942711\n",
      "Updating alpha to: 2508541023.4713554\n",
      "Updating alpha to: 12542705117.356777\n",
      "Updating alpha to: 62713525586.78389\n",
      "Updating alpha to: 47.333839416503906\n",
      "Updating alpha to: 50.71088981628418\n",
      "Updating rho, dag penalty prev:  3.3770503998\n",
      "Updating rho, dag penalty prev:  3.3770503998\n",
      "Updating rho, dag penalty prev:  3.3770503998\n",
      "Updating rho, dag penalty prev:  3.3770503998\n",
      "Updating rho, dag penalty prev:  3.3770503998\n",
      "Updating rho, dag penalty prev:  3.3770503998\n",
      "Updating alpha to: 1923848.3183116913\n",
      "Updating alpha to: 2132306.265089035\n",
      "Updating alpha to: 10661531.325445175\n",
      "Updating alpha to: 53307656.627225876\n",
      "Updating alpha to: 36.916908264160156\n",
      "Updating alpha to: 45.36589813232422\n",
      "Updating rho, dag penalty prev:  8.4489898682\n",
      "Updating alpha to: 67.19960403442383\n",
      "Updating rho, dag penalty prev:  2.1833705902\n",
      "Updating rho, dag penalty prev:  2.1833705902\n",
      "Updating rho, dag penalty prev:  2.1833705902\n",
      "Updating rho, dag penalty prev:  2.1833705902\n",
      "Updating rho, dag penalty prev:  2.1833705902\n",
      "Updating rho, dag penalty prev:  2.1833705902\n",
      "Updating rho, dag penalty prev:  2.1833705902\n",
      "Updating alpha to: 16676588.500873566\n",
      "Updating alpha to: 83382942.50436783\n",
      "Updating alpha to: 416914712.52183914\n",
      "Updating alpha to: 2084573562.6091957\n",
      "Updating alpha to: 10422867813.045979\n",
      "Updating alpha to: 52114339065.22989\n",
      "Updating alpha to: 42.2890625\n",
      "Updating alpha to: 61.230255126953125\n",
      "Updating rho, dag penalty prev:  18.9411926270\n",
      "Updating alpha to: 183.80838012695312\n",
      "Updating rho, dag penalty prev:  12.2578125000\n",
      "Updating rho, dag penalty prev:  12.2578125000\n",
      "Updating rho, dag penalty prev:  12.2578125000\n",
      "Updating rho, dag penalty prev:  12.2578125000\n",
      "Updating rho, dag penalty prev:  12.2578125000\n",
      "Updating rho, dag penalty prev:  12.2578125000\n",
      "Updating alpha to: 75373184.90701294\n",
      "Updating alpha to: 80414917.69509888\n",
      "Updating alpha to: 402074588.4754944\n",
      "Updating alpha to: 2010372942.377472\n",
      "Updating alpha to: 10051864711.88736\n",
      "Updating alpha to: 50259323559.4368\n",
      "Updating alpha to: 251296617797.184\n",
      "Updating alpha to: 32.548126220703125\n",
      "Updating alpha to: 49.11301803588867\n",
      "Updating alpha to: 59.35786247253418\n",
      "Updating rho, dag penalty prev:  10.2448444366\n",
      "Updating rho, dag penalty prev:  10.2448444366\n",
      "Updating rho, dag penalty prev:  10.2448444366\n",
      "Updating rho, dag penalty prev:  10.2448444366\n",
      "Updating rho, dag penalty prev:  10.2448444366\n",
      "Updating rho, dag penalty prev:  10.2448444366\n",
      "Updating rho, dag penalty prev:  10.2448444366\n",
      "Updating alpha to: 30890810.24287224\n",
      "Updating alpha to: 154454051.2143612\n",
      "Updating alpha to: 772270256.071806\n",
      "Updating alpha to: 3861351280.35903\n",
      "Updating alpha to: 19306756401.79515\n",
      "Updating alpha to: 96533782008.97575\n",
      "Updating alpha to: 97.02572631835938\n",
      "Updating alpha to: 108.0193977355957\n",
      "Updating alpha to: 114.77030563354492\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating rho, dag penalty prev:  6.7509078979\n",
      "Updating alpha to: 243938942.28495407\n",
      "Updating alpha to: 294356270.16581345\n",
      "Updating alpha to: 1471781350.8290672\n",
      "Updating alpha to: 7358906754.145336\n",
      "Updating alpha to: 36794533770.726685\n",
      "Updating alpha to: 183972668853.63342\n",
      "Updating alpha to: 919863344268.1671\n",
      "Updating alpha to: 26.259490966796875\n",
      "Updating alpha to: 31.47685432434082\n",
      "Updating rho, dag penalty prev:  5.2173633575\n",
      "Updating rho, dag penalty prev:  5.2173633575\n",
      "Updating rho, dag penalty prev:  5.2173633575\n",
      "Updating rho, dag penalty prev:  5.2173633575\n",
      "Updating rho, dag penalty prev:  5.2173633575\n",
      "index 8 is out of bounds for dimension 1 with size 8\n",
      "Updating alpha to: 197.13189697265625\n",
      "Updating alpha to: 198.79923629760742\n",
      "Updating alpha to: 199.7304801940918\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating rho, dag penalty prev:  0.9312438965\n",
      "Updating alpha to: 546041688.3779411\n",
      "Updating rho, dag penalty prev:  0.5460414886\n",
      "Updating rho, dag penalty prev:  0.5460414886\n",
      "Updating alpha to: 2730208441.8897057\n",
      "Updating alpha to: 13651042209.448528\n",
      "Updating alpha to: 68255211047.242645\n",
      "Updating alpha to: 341276055236.21326\n",
      "Updating alpha to: 1706380276181.0664\n",
      "Updating alpha to: 75.13719177246094\n",
      "Updating alpha to: 89.0718765258789\n",
      "Updating rho, dag penalty prev:  13.9346847534\n",
      "Updating rho, dag penalty prev:  13.9346847534\n",
      "Updating rho, dag penalty prev:  13.9346847534\n",
      "Updating rho, dag penalty prev:  13.9346847534\n",
      "Updating rho, dag penalty prev:  13.9346847534\n",
      "Updating rho, dag penalty prev:  13.9346847534\n",
      "Updating rho, dag penalty prev:  13.9346847534\n",
      "Updating alpha to: 52325604.81894684\n",
      "Updating alpha to: 261628024.0947342\n",
      "Updating alpha to: 1308140120.473671\n",
      "Updating alpha to: 6540700602.368355\n",
      "Updating alpha to: 32703503011.841774\n",
      "Updating alpha to: 163517515059.20886\n",
      "Expected parameter logits_exist (Parameter of shape (17, 17)) of distribution ENCOAdjacencyDistribution(logits_exist: torch.Size([17, 17]), logits_orient: torch.Size([136])) to satisfy the constraint Real(), but found invalid values:\n",
      "Parameter containing:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       requires_grad=True)\n",
      "Updating alpha to: 39.16259002685547\n",
      "Updating alpha to: 47.45958137512207\n",
      "Updating rho, dag penalty prev:  8.2969913483\n",
      "Updating rho, dag penalty prev:  8.2969913483\n",
      "Updating alpha to: 358.1609516143799\n",
      "Updating rho, dag penalty prev:  3.1070137024\n",
      "Updating rho, dag penalty prev:  3.1070137024\n",
      "Updating rho, dag penalty prev:  3.1070137024\n",
      "Updating alpha to: 186220.129945755\n",
      "Updating alpha to: 202896.65124702454\n",
      "Updating alpha to: 1014483.2562351227\n",
      "Updating alpha to: 5072416.281175613\n",
      "Updating alpha to: 28.816482543945312\n",
      "Updating alpha to: 35.25055122375488\n",
      "Updating alpha to: 37.708810806274414\n",
      "Updating rho, dag penalty prev:  2.4582595825\n",
      "Updating rho, dag penalty prev:  2.4582595825\n",
      "Updating rho, dag penalty prev:  2.4582595825\n",
      "Updating rho, dag penalty prev:  2.4582595825\n",
      "Updating rho, dag penalty prev:  2.4582595825\n",
      "Updating rho, dag penalty prev:  2.4582595825\n",
      "Updating rho, dag penalty prev:  2.4582595825\n",
      "Updating alpha to: 5041770.496896744\n",
      "Updating alpha to: 25208852.48448372\n",
      "Updating alpha to: 126044262.4224186\n",
      "Updating alpha to: 630221312.112093\n",
      "Updating alpha to: 3151106560.560465\n",
      "Updating alpha to: 15755532802.802324\n",
      "Updating alpha to: 76.25665283203125\n",
      "Updating alpha to: 80.58177757263184\n",
      "Updating rho, dag penalty prev:  4.3251247406\n",
      "Updating alpha to: 102.04074668884277\n",
      "Updating rho, dag penalty prev:  2.1458969116\n",
      "Updating rho, dag penalty prev:  2.1458969116\n",
      "Updating rho, dag penalty prev:  2.1458969116\n",
      "Updating rho, dag penalty prev:  2.1458969116\n",
      "Updating rho, dag penalty prev:  2.1458969116\n",
      "Updating rho, dag penalty prev:  2.1458969116\n",
      "Updating rho, dag penalty prev:  2.1458969116\n",
      "Updating alpha to: 122837931.63059044\n",
      "Updating alpha to: 173255259.5114498\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 108284537194.65614\n",
      "Updating alpha to: 2707113429866.4033\n",
      "Updating alpha to: 50000000000000.0\n",
      "Updating alpha to: 50000000000000.0\n",
      "Updating alpha to: 50000000000000.0\n",
      "Updating alpha to: 104.07725524902344\n",
      "Updating alpha to: 113.5161190032959\n",
      "Updating rho, dag penalty prev:  9.4388637543\n",
      "Updating rho, dag penalty prev:  9.4388637543\n",
      "Updating rho, dag penalty prev:  9.4388637543\n",
      "Updating rho, dag penalty prev:  9.4388637543\n",
      "Updating rho, dag penalty prev:  9.4388637543\n",
      "Updating rho, dag penalty prev:  9.4388637543\n",
      "Updating alpha to: 5453961.401250839\n",
      "Updating alpha to: 6303013.830450058\n",
      "Updating alpha to: 6636742.620733261\n",
      "Updating alpha to: 6803505.926397324\n",
      "Updating alpha to: 34017529.63198662\n",
      "Updating alpha to: 170087648.1599331\n",
      "Updating alpha to: 850438240.7996655\n",
      "Updating alpha to: 4252191203.9983273\n",
      "Updating alpha to: 21260956019.99164\n",
      "Updating alpha to: 69.32862854003906\n",
      "Updating alpha to: 76.18995666503906\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating rho, dag penalty prev:  6.8613281250\n",
      "Updating alpha to: 180167846.57569885\n",
      "Updating alpha to: 230585174.45655823\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 1152925872.2827911\n",
      "Updating alpha to: 5764629361.413956\n",
      "Updating alpha to: 28823146807.06978\n",
      "Updating alpha to: 144115734035.34888\n",
      "Updating alpha to: 720578670176.7444\n",
      "Updating alpha to: 80.30258178710938\n",
      "Updating alpha to: 85.70035934448242\n",
      "Updating rho, dag penalty prev:  5.3977775574\n",
      "Updating alpha to: 114.80207443237305\n",
      "Updating rho, dag penalty prev:  2.9101715088\n",
      "Updating rho, dag penalty prev:  2.9101715088\n",
      "Updating rho, dag penalty prev:  2.9101715088\n",
      "Updating rho, dag penalty prev:  2.9101715088\n",
      "Updating rho, dag penalty prev:  2.9101715088\n",
      "Updating alpha to: 983291.9871330261\n",
      "Updating alpha to: 4916459.935665131\n",
      "Updating alpha to: 24582299.678325653\n",
      "Updating alpha to: 122911498.39162827\n",
      "Updating alpha to: 614557491.9581413\n",
      "Updating alpha to: 3072787459.7907066\n",
      "Updating alpha to: 64.49445343017578\n",
      "Updating alpha to: 69.0043716430664\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating rho, dag penalty prev:  4.5099182129\n",
      "Updating alpha to: 50417396.88523102\n",
      "Updating alpha to: 252086984.4261551\n",
      "Updating alpha to: 1260434922.1307755\n",
      "Updating alpha to: 6302174610.653877\n",
      "Updating alpha to: 31510873053.269386\n",
      "Updating alpha to: 157554365266.34692\n",
      "Updating alpha to: 8.90934944152832\n",
      "Updating alpha to: 11.591480255126953\n",
      "Updating rho, dag penalty prev:  2.6821308136\n",
      "Updating alpha to: 25.921550750732422\n",
      "Updating alpha to: 28.77356719970703\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating rho, dag penalty prev:  2.8520164490\n",
      "Updating alpha to: 140654783.41223907\n",
      "Updating alpha to: 703273917.0611954\n",
      "Updating alpha to: 3516369585.305977\n",
      "Updating alpha to: 17581847926.529884\n",
      "Updating alpha to: 87909239632.64941\n",
      "Updating alpha to: 439546198163.2471\n",
      "Updating alpha to: 112.48129272460938\n",
      "Updating alpha to: 115.99052238464355\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "Updating rho, dag penalty prev:  3.5092296600\n",
      "index -1 is out of bounds for dimension 1 with size 9\n",
      "Updating alpha to: 74.65205383300781\n",
      "Updating alpha to: 80.7722053527832\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating rho, dag penalty prev:  6.1201515198\n",
      "Updating alpha to: 245464787.1931038\n",
      "Updating alpha to: 1227323935.965519\n",
      "Updating alpha to: 6136619679.827595\n",
      "Updating alpha to: 30683098399.137974\n",
      "Updating alpha to: 153415491995.68988\n",
      "Updating alpha to: 767077459978.4495\n",
      "Updating alpha to: 40.91322708129883\n",
      "Updating alpha to: 43.32295799255371\n",
      "Updating rho, dag penalty prev:  2.4097309113\n",
      "Updating rho, dag penalty prev:  2.4097309113\n",
      "Updating rho, dag penalty prev:  2.4097309113\n",
      "Updating rho, dag penalty prev:  2.4097309113\n",
      "Updating alpha to: 14965.159811019897\n",
      "Updating rho, dag penalty prev:  1.4921836853\n",
      "Updating alpha to: 65382.48769187927\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 326912.43845939636\n",
      "Updating alpha to: 1634562.1922969818\n",
      "Updating alpha to: 8172810.961484909\n",
      "Updating alpha to: 40864054.807424545\n",
      "Updating alpha to: 204320274.03712273\n",
      "Updating alpha to: 100.36198425292969\n",
      "Updating alpha to: 107.20511245727539\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating rho, dag penalty prev:  6.8431282043\n",
      "Updating alpha to: 200918304.8369484\n",
      "Updating alpha to: 327965462.12454605\n",
      "Updating alpha to: 378382790.0054054\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 1891913950.0270271\n",
      "Updating alpha to: 9459569750.135136\n",
      "Updating alpha to: 47297848750.675674\n",
      "Updating alpha to: 236489243753.37836\n",
      "Updating alpha to: 1182446218766.8918\n",
      "Updating alpha to: 64.42247009277344\n",
      "Updating alpha to: 75.9556827545166\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating rho, dag penalty prev:  11.5332126617\n",
      "Updating alpha to: 4077066497.464472\n",
      "Updating alpha to: 20385332487.322357\n",
      "Updating alpha to: 101926662436.61179\n",
      "Updating alpha to: 509633312183.05896\n",
      "Updating alpha to: 2548166560915.295\n",
      "Updating alpha to: 12740832804576.475\n",
      "Updating alpha to: 9.852306365966797\n",
      "Updating alpha to: 10.35647964477539\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 5.440826416015625\n",
      "Updating alpha to: 7.236419677734375\n",
      "Updating rho, dag penalty prev:  1.7955932617\n",
      "Updating rho, dag penalty prev:  1.7955932617\n",
      "Updating alpha to: 113.09388732910156\n",
      "Updating rho, dag penalty prev:  1.0585746765\n",
      "Updating alpha to: 617.2671661376953\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 60.2747802734375\n",
      "Updating alpha to: 62.2476692199707\n",
      "Updating rho, dag penalty prev:  1.9728889465\n",
      "Updating rho, dag penalty prev:  1.9728889465\n",
      "Updating rho, dag penalty prev:  1.9728889465\n",
      "Updating rho, dag penalty prev:  1.9728889465\n",
      "Updating alpha to: 10741.454517364502\n",
      "index 8 is out of bounds for dimension 1 with size 8\n",
      "Updating alpha to: 64.34569549560547\n",
      "Updating alpha to: 79.59307861328125\n",
      "Updating rho, dag penalty prev:  15.2473831177\n",
      "Updating rho, dag penalty prev:  15.2473831177\n",
      "Updating rho, dag penalty prev:  15.2473831177\n",
      "Updating rho, dag penalty prev:  15.2473831177\n",
      "Updating rho, dag penalty prev:  15.2473831177\n",
      "Updating rho, dag penalty prev:  15.2473831177\n",
      "Updating alpha to: 3635421.2373657227\n",
      "Updating alpha to: 18177106.186828613\n",
      "Updating alpha to: 90885530.93414307\n",
      "Updating alpha to: 454427654.67071533\n",
      "Updating alpha to: 2272138273.3535767\n",
      "Updating alpha to: 11360691366.767883\n",
      "Updating alpha to: 47.311859130859375\n",
      "Updating alpha to: 57.57295608520508\n",
      "Updating alpha to: 60.79337692260742\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating rho, dag penalty prev:  3.2204208374\n",
      "Updating alpha to: 187983955.14152145\n",
      "Updating alpha to: 238401283.02238083\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating rho, dag penalty prev:  0.5041732788\n",
      "Updating alpha to: 5960032075.559521\n",
      "Updating alpha to: 149000801888.988\n",
      "Updating alpha to: 3725020047224.7\n",
      "Updating alpha to: 50000000000000.0\n",
      "Updating alpha to: 50000000000000.0\n",
      "Updating alpha to: 108.15521240234375\n",
      "Updating alpha to: 120.52828598022461\n",
      "Updating alpha to: 125.04333877563477\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating rho, dag penalty prev:  4.5150527954\n",
      "Updating alpha to: 166763430.70740128\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_metadata = []\n",
    "iteration = 0 \n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "previous_fail = False\n",
    "for i, row in enumerate(cats_rows_list):\n",
    "    if previous_fail == True:\n",
    "        device =  'cpu'\n",
    "    else: \n",
    "        device = 'cuda:0'\n",
    "    try:\n",
    "        if i == 0:       \n",
    "            start_time = datetime.strptime(row['start_time'],'%Y-%m-%d %H:%M:%S')\n",
    "            end_time = datetime.strptime(row['end_time'],'%Y-%m-%d %H:%M:%S')\n",
    "            delta = end_time - start_time\n",
    "            start_time = start_time - delta\n",
    "        else:\n",
    "            start_time = end_time + timedelta(seconds=1)\n",
    "            end_time = datetime.strptime(row['end_time'],'%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "        \n",
    "        anomaly = eval(row['affected'])[0]\n",
    "        root_cause = row['root_cause']\n",
    "        \n",
    "        model_df = cats_df.filter( (pl.col('timestamp')>= start_time) & (pl.col('timestamp') <= end_time))\n",
    "        model_df = model_df.drop(['timestamp','y','category'])\n",
    "        out_cols = model_df.columns\n",
    "        cats_np = model_df.to_numpy()\n",
    "    \n",
    "        # Cast data to torch tensors\n",
    "        data_tensors = {}\n",
    "        \n",
    "        for i in range(cats_np.shape[1]):\n",
    "            data_tensors[out_cols[i]] = torch.tensor(cats_np[:, i].reshape(-1, 1))\n",
    "            \n",
    "        dataset_train = TensorDict(data_tensors, torch.Size([cats_np.shape[0]]))\n",
    "            \n",
    "        # Move the entire dataset to the device (for big datasets move to device by batch within training loop)\n",
    "        dataset_train = dataset_train.apply(lambda t: t.to(dtype=torch.float32, device=device)).to(device)\n",
    "        \n",
    "        \n",
    "        # Create loader\n",
    "        dataloader_train = DataLoader(\n",
    "            dataset=dataset_train,\n",
    "            collate_fn=lambda x: x,\n",
    "            batch_size=training_config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "    \n",
    "        num_nodes = len(dataset_train.keys())\n",
    "    \n",
    "        # Define the prior\n",
    "        prior = cd.GibbsDAGPrior(\n",
    "            num_nodes=num_nodes, \n",
    "            sparsity_lambda=training_config.prior_sparsity_lambda,\n",
    "           # expert_graph_container=expert_knowledge\n",
    "        )\n",
    "    \n",
    "            # Define the adjaceny module\n",
    "        adjacency_dist = cd.ENCOAdjacencyDistributionModule(num_nodes)\n",
    "        \n",
    "        #Define the functional module\n",
    "        icgnn = ICGNN(\n",
    "            variables=tensordict_shapes(dataset_train),\n",
    "            embedding_size=8, #32,\n",
    "            out_dim_g=8, #32,\n",
    "            norm_layer=torch.nn.LayerNorm,\n",
    "            res_connection=True,\n",
    "        )\n",
    "        \n",
    "        # Define the noise module\n",
    "        types_dict = {var_name: VariableTypeEnum.CONTINUOUS for var_name in dataset_train.keys()}\n",
    "        \n",
    "        noise_submodules = cd.create_noise_modules(\n",
    "            shapes=tensordict_shapes(dataset_train), \n",
    "            types=types_dict, \n",
    "            continuous_noise_dist=training_config.noise_dist\n",
    "        )\n",
    "        \n",
    "        noise_module = cd.JointNoiseModule(noise_submodules)\n",
    "    \n",
    "        sem_module = cd.SEMDistributionModule(\n",
    "        adjacency_module=adjacency_dist, \n",
    "        functional_relationships=icgnn, \n",
    "        noise_module=noise_module)\n",
    "    \n",
    "        sem_module.to(device)\n",
    "    \n",
    "        modules = {\n",
    "        \"icgnn\": sem_module.functional_relationships,\n",
    "        \"vardist\": sem_module.adjacency_module,\n",
    "        \"noise_dist\": sem_module.noise_module,\n",
    "        }\n",
    "        \n",
    "        parameter_list = [\n",
    "            {\"params\": module.parameters(), \"lr\": auglag_config.lr_init_dict[name], \"name\": name}\n",
    "            for name, module in modules.items()\n",
    "        ]\n",
    "        \n",
    "        # Define the optimizer\n",
    "        optimizer = torch.optim.Adam(parameter_list)\n",
    "                \n",
    "        \n",
    "    \n",
    "        # Define the augmented Lagrangian loss objects\n",
    "        scheduler = AugLagLR(config=auglag_config)\n",
    "        \n",
    "        auglag_loss = AugLagLossCalculator(\n",
    "            init_alpha=training_config.init_alpha, \n",
    "            init_rho=training_config.init_rho\n",
    "        )\n",
    "    \n",
    "        assert len(dataset_train.batch_size) == 1, \"Only 1D batch size is supported\"\n",
    "    \n",
    "        num_samples = len(dataset_train)\n",
    "        \n",
    "        for epoch in range(training_config.max_epoch):\n",
    "            \n",
    "            for i, batch in enumerate(dataloader_train):\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Get SEM \n",
    "                sem_distribution = sem_module()\n",
    "                sem, *_ = sem_distribution.relaxed_sample(\n",
    "                    torch.Size([]), \n",
    "                    temperature=training_config.gumbel_temp\n",
    "                )  # soft sample\n",
    "                \n",
    "                # Compute the log probability of data\n",
    "                batch_log_prob = sem.log_prob(batch).mean()\n",
    "                \n",
    "                # Get the distribution entropy\n",
    "                sem_distribution_entropy = sem_distribution.entropy()\n",
    "                \n",
    "                # Compute the likelihood of the current graph\n",
    "                prior_term = prior.log_prob(sem.graph.to(device))\n",
    "                \n",
    "                # Compute the objective\n",
    "                objective = (-sem_distribution_entropy - prior_term) / num_samples - batch_log_prob\n",
    "                \n",
    "                # Compute the DAG-ness term\n",
    "                constraint = calculate_dagness(sem.graph)\n",
    "                \n",
    "                # Compute the Lagrangian loss\n",
    "                loss = auglag_loss(objective, constraint / num_samples)\n",
    "        \n",
    "                # Propagate gradients and update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update the Auglag parameters\n",
    "                scheduler.step(\n",
    "                    optimizer=optimizer,\n",
    "                    loss=auglag_loss,\n",
    "                    loss_value=loss.item(),\n",
    "                    lagrangian_penalty=constraint.item(),\n",
    "                )\n",
    "                \n",
    "                # Log metrics & plot the matrices\n",
    "                \"\"\"if epoch % 500 == 0 and i == 0:\n",
    "                    print(\n",
    "                        f\"epoch:{epoch} loss:{loss.item():.5g} nll:{-batch_log_prob.detach().cpu().numpy():.5g} \"\n",
    "                        f\"dagness:{constraint.item():.5f} num_edges:{(sem.graph > 0.0).sum()} \"\n",
    "                        f\"alpha:{auglag_loss.alpha:.5g} rho:{auglag_loss.rho:.5g} \"\n",
    "                        f\"step:{scheduler.outer_opt_counter}|{scheduler.step_counter} \"\n",
    "                        f\"num_lr_updates:{scheduler.num_lr_updates}\"\n",
    "                    )\"\"\"\n",
    "    \n",
    "        vardist = adjacency_dist()\n",
    "        pred_dag = vardist.mode.cpu().numpy()\n",
    "    \n",
    "        treatment_columns = set(out_cols)\n",
    "        treatment_columns.remove(anomaly)\n",
    "        treatment_columns = list(treatment_columns)\n",
    "    \n",
    "        estimated_ate = {}\n",
    "        num_samples = 1000\n",
    "        sample_shape = torch.Size([num_samples])\n",
    "        #normalizer = data_module.normalizer\n",
    "    \n",
    "        estimated_ate = {}\n",
    "        num_samples = 20000\n",
    "        sample_shape = torch.Size([num_samples])\n",
    "        #normalizer = data_module.normalizer\n",
    "        \n",
    "        for treatment in treatment_columns:\n",
    "            intervention_a = TensorDict({treatment: torch.tensor([1.0]).to(device)}, batch_size=tuple())\n",
    "            intervention_b = TensorDict({treatment: torch.tensor([0.0]).to(device)}, batch_size=tuple())\n",
    "        \n",
    "            rev_a_samples = (sem.do(interventions=intervention_a).sample(sample_shape))[anomaly]\n",
    "            rev_b_samples = (sem.do(interventions=intervention_b).sample(sample_shape))[anomaly]\n",
    "        \n",
    "            ate_mean = rev_a_samples.mean(0) - rev_b_samples.mean(0)\n",
    "            ate_std = np.sqrt((rev_a_samples.cpu().var(0) + rev_b_samples.cpu().var(0)) / num_samples)\n",
    "        \n",
    "            estimated_ate[treatment] = (\n",
    "                ate_mean.cpu().numpy()[0],\n",
    "                ate_std.cpu().numpy()[0],\n",
    "            )\n",
    "        \n",
    "        col_names = []\n",
    "        effects = []\n",
    "        for k, effect in estimated_ate.items():\n",
    "            col_names.append(k)\n",
    "            effects.append(np.abs(effect[0]))  \n",
    "    \n",
    "        top_causes = pd.DataFrame({\"variable\":col_names,'effect':effects}).sort_values(by='effect', ascending=False)[0:3]['variable'].reset_index(drop=True)\n",
    "    \n",
    "        if root_cause == top_causes[0]:\n",
    "            row['cause_1'] = 1\n",
    "        if root_cause == top_causes[1]:\n",
    "            row['cause_2'] = 1\n",
    "        if root_cause == top_causes[2]:\n",
    "            row['cause_3'] = 1\n",
    "        new_metadata.append(row)\n",
    "        if iteration%50 == 0:\n",
    "            print(iteration)\n",
    "        iteration+=1\n",
    "\n",
    "\n",
    "        del sem\n",
    "        del intervention_a\n",
    "        del intervention_b\n",
    "        del dataset_train\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        previous_fail = True\n",
    "        print(e)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90267640-601a-41fd-b39c-598b8342bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80572339-9399-4cec-bcc4-3755f55ee735",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_ate = {}\n",
    "num_samples = 20000\n",
    "sample_shape = torch.Size([num_samples])\n",
    "#normalizer = data_module.normalizer\n",
    "\n",
    "for treatment in treatment_columns:\n",
    "    intervention_a = TensorDict({treatment: torch.tensor([1.0]).to(device)}, batch_size=tuple())\n",
    "    intervention_b = TensorDict({treatment: torch.tensor([0.0]).to(device)}, batch_size=tuple())\n",
    "\n",
    "    rev_a_samples = (sem.do(interventions=intervention_a).sample(sample_shape))[anomaly]\n",
    "    rev_b_samples = (sem.do(interventions=intervention_b).sample(sample_shape))[anomaly]\n",
    "\n",
    "    ate_mean = rev_a_samples.mean(0) - rev_b_samples.mean(0)\n",
    "    ate_std = np.sqrt((rev_a_samples.cpu().var(0) + rev_b_samples.cpu().var(0)) / num_samples)\n",
    "\n",
    "    estimated_ate[treatment] = (\n",
    "        ate_mean.cpu().numpy()[0],\n",
    "        ate_std.cpu().numpy()[0],\n",
    "    )\n",
    "estimated_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ddb75-5b68-4289-954c-f91ff136a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "effects = []\n",
    "for k, effect in estimated_ate.items():\n",
    "    col_names.append(k)\n",
    "    effects.append(np.abs(effect[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9a27b-fa7f-40fa-95cf-407d454d5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_causes = pd.DataFrame({\"variable\":col_names,'effect':effects}).sort_values(by='effect', ascending=False)[0:3]['variable'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18229fb5-b4cf-4345-b410-c3f87e8f3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_causes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cfdcf-0f73-44b9-8042-7b87e8be983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top_causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc1451-920d-446d-b1ab-cf552649b4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
