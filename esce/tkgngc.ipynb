{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3194ece4-dae8-4863-9042-51422221471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkgngc.embeddings import PretrainedTKGEmbeddingWithTimestamps\n",
    "from tkgngc.model import NGCWithPretrainedTKGAndTimestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b418066-4a1b-4b6e-8310-2cb3c77b4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from feature.scalers import ranged_scaler\n",
    "from datetime import datetime, timedelta\n",
    "#from mpge.rca import mpge_root_cause_diagnosis\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20716e6-c787-4b60-9882-53d8447acd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0492a0de-7680-430d-ad2e-95c5f2648bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = pl.read_csv(\"data/data.csv\", separator=\",\")  \n",
    "metadata = pl.read_csv('data/metadata.csv',separator=',')\n",
    "potential_causes = metadata['root_cause'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb1e334-db07-4cba-a71c-aed9f26dab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cats_df.columns:\n",
    "    unique_vals = cats_df[col].n_unique()\n",
    "    data_type = cats_df[col].dtype\n",
    "    bad_dtypes = [pl.Date,pl.Datetime,pl.Utf8]\n",
    "    if ((unique_vals >= 50) & (data_type not in bad_dtypes) ):\n",
    "        cats_df = cats_df.with_columns(ranged_scaler(cats_df[col]))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72851f4a-456f-45d9-a897-e1cc5aebb938",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_df = cats_df.with_columns(\n",
    "    pl.col('timestamp').str.to_datetime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    pl.Series(\"entity_id\",range(len(cats_df)))\n",
    ")\n",
    "cats_rows_list = metadata.rows(named=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b5c14e-8c6f-4a16-af2c-fb2959751ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>aimp</th><th>amud</th><th>arnd</th><th>asin1</th><th>asin2</th><th>adbr</th><th>adfl</th><th>bed1</th><th>bed2</th><th>bfo1</th><th>bfo2</th><th>bso1</th><th>bso2</th><th>bso3</th><th>ced1</th><th>cfo1</th><th>cso1</th><th>y</th><th>category</th><th>entity_id</th></tr><tr><td>datetime[μs]</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>2023-01-01 00:00:00</td><td>0.0</td><td>0.142857</td><td>-0.5</td><td>-4.1078e-14</td><td>2.0428e-14</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180547</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100389</td><td>-0.186623</td><td>0.0</td><td>0.0</td><td>0</td></tr><tr><td>2023-01-01 00:00:01</td><td>0.0</td><td>0.142857</td><td>-0.495998</td><td>0.00002</td><td>0.0002</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.18054</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100389</td><td>-0.186618</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>2023-01-01 00:00:02</td><td>0.0</td><td>0.142857</td><td>-0.486172</td><td>0.00004</td><td>0.0004</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180519</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.10039</td><td>-0.186604</td><td>0.0</td><td>0.0</td><td>2</td></tr><tr><td>2023-01-01 00:00:03</td><td>0.0</td><td>0.142857</td><td>-0.463453</td><td>0.00006</td><td>0.0006</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180484</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100391</td><td>-0.18658</td><td>0.0</td><td>0.0</td><td>3</td></tr><tr><td>2023-01-01 00:00:04</td><td>0.0</td><td>0.142857</td><td>-0.444095</td><td>0.00008</td><td>0.0008</td><td>0.0</td><td>0.0</td><td>-0.32802</td><td>-0.369237</td><td>-0.738163</td><td>-0.767181</td><td>-0.180437</td><td>-0.507953</td><td>-0.716059</td><td>-0.774361</td><td>0.100393</td><td>-0.186548</td><td>0.0</td><td>0.0</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 21)\n",
       "┌─────────────────────┬──────┬──────────┬───────────┬───┬───────────┬─────┬──────────┬───────────┐\n",
       "│ timestamp           ┆ aimp ┆ amud     ┆ arnd      ┆ … ┆ cso1      ┆ y   ┆ category ┆ entity_id │\n",
       "│ ---                 ┆ ---  ┆ ---      ┆ ---       ┆   ┆ ---       ┆ --- ┆ ---      ┆ ---       │\n",
       "│ datetime[μs]        ┆ f64  ┆ f64      ┆ f64       ┆   ┆ f64       ┆ f64 ┆ f64      ┆ i64       │\n",
       "╞═════════════════════╪══════╪══════════╪═══════════╪═══╪═══════════╪═════╪══════════╪═══════════╡\n",
       "│ 2023-01-01 00:00:00 ┆ 0.0  ┆ 0.142857 ┆ -0.5      ┆ … ┆ -0.186623 ┆ 0.0 ┆ 0.0      ┆ 0         │\n",
       "│ 2023-01-01 00:00:01 ┆ 0.0  ┆ 0.142857 ┆ -0.495998 ┆ … ┆ -0.186618 ┆ 0.0 ┆ 0.0      ┆ 1         │\n",
       "│ 2023-01-01 00:00:02 ┆ 0.0  ┆ 0.142857 ┆ -0.486172 ┆ … ┆ -0.186604 ┆ 0.0 ┆ 0.0      ┆ 2         │\n",
       "│ 2023-01-01 00:00:03 ┆ 0.0  ┆ 0.142857 ┆ -0.463453 ┆ … ┆ -0.18658  ┆ 0.0 ┆ 0.0      ┆ 3         │\n",
       "│ 2023-01-01 00:00:04 ┆ 0.0  ┆ 0.142857 ┆ -0.444095 ┆ … ┆ -0.186548 ┆ 0.0 ┆ 0.0      ┆ 4         │\n",
       "└─────────────────────┴──────┴──────────┴───────────┴───┴───────────┴─────┴──────────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_rows_list = metadata.rows(named=True)\n",
    "cats_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d14502-5f9b-4113-a703-90a48e022b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cats_df = cats_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d67caa5-fd5a-42b0-9d06-2a8148f86eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>aimp</th>\n",
       "      <th>amud</th>\n",
       "      <th>arnd</th>\n",
       "      <th>asin1</th>\n",
       "      <th>asin2</th>\n",
       "      <th>adbr</th>\n",
       "      <th>adfl</th>\n",
       "      <th>bed1</th>\n",
       "      <th>bed2</th>\n",
       "      <th>...</th>\n",
       "      <th>bfo2</th>\n",
       "      <th>bso1</th>\n",
       "      <th>bso2</th>\n",
       "      <th>bso3</th>\n",
       "      <th>ced1</th>\n",
       "      <th>cfo1</th>\n",
       "      <th>cso1</th>\n",
       "      <th>y</th>\n",
       "      <th>category</th>\n",
       "      <th>entity_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-4.107825e-14</td>\n",
       "      <td>2.042810e-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>-0.180547</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>-0.774361</td>\n",
       "      <td>0.100389</td>\n",
       "      <td>-0.186623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.495998</td>\n",
       "      <td>2.000000e-05</td>\n",
       "      <td>2.000000e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>-0.180540</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>-0.774361</td>\n",
       "      <td>0.100389</td>\n",
       "      <td>-0.186618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:00:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.486172</td>\n",
       "      <td>4.000000e-05</td>\n",
       "      <td>4.000000e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>-0.180519</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>-0.774361</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>-0.186604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:00:03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.463453</td>\n",
       "      <td>6.000000e-05</td>\n",
       "      <td>6.000000e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>-0.180484</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>-0.774361</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>-0.186580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:00:04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.444095</td>\n",
       "      <td>8.000000e-05</td>\n",
       "      <td>7.999999e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.32802</td>\n",
       "      <td>-0.369237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.767181</td>\n",
       "      <td>-0.180437</td>\n",
       "      <td>-0.507953</td>\n",
       "      <td>-0.716059</td>\n",
       "      <td>-0.774361</td>\n",
       "      <td>0.100393</td>\n",
       "      <td>-0.186548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  aimp      amud      arnd         asin1         asin2  \\\n",
       "0 2023-01-01 00:00:00   0.0  0.142857 -0.500000 -4.107825e-14  2.042810e-14   \n",
       "1 2023-01-01 00:00:01   0.0  0.142857 -0.495998  2.000000e-05  2.000000e-04   \n",
       "2 2023-01-01 00:00:02   0.0  0.142857 -0.486172  4.000000e-05  4.000000e-04   \n",
       "3 2023-01-01 00:00:03   0.0  0.142857 -0.463453  6.000000e-05  6.000000e-04   \n",
       "4 2023-01-01 00:00:04   0.0  0.142857 -0.444095  8.000000e-05  7.999999e-04   \n",
       "\n",
       "   adbr  adfl     bed1      bed2  ...      bfo2      bso1      bso2      bso3  \\\n",
       "0   0.0   0.0 -0.32802 -0.369237  ... -0.767181 -0.180547 -0.507953 -0.716059   \n",
       "1   0.0   0.0 -0.32802 -0.369237  ... -0.767181 -0.180540 -0.507953 -0.716059   \n",
       "2   0.0   0.0 -0.32802 -0.369237  ... -0.767181 -0.180519 -0.507953 -0.716059   \n",
       "3   0.0   0.0 -0.32802 -0.369237  ... -0.767181 -0.180484 -0.507953 -0.716059   \n",
       "4   0.0   0.0 -0.32802 -0.369237  ... -0.767181 -0.180437 -0.507953 -0.716059   \n",
       "\n",
       "       ced1      cfo1      cso1    y  category  entity_id  \n",
       "0 -0.774361  0.100389 -0.186623  0.0       0.0          0  \n",
       "1 -0.774361  0.100389 -0.186618  0.0       0.0          1  \n",
       "2 -0.774361  0.100390 -0.186604  0.0       0.0          2  \n",
       "3 -0.774361  0.100391 -0.186580  0.0       0.0          3  \n",
       "4 -0.774361  0.100393 -0.186548  0.0       0.0          4  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2223bcf6-5492-4152-9bd4-e0784d33b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78459ad4-7810-4602-93c5-79fff5a0e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cats_df[0:1000000]\n",
    "train = train_df[['aimp', 'amud', 'arnd', 'asin1', 'asin2', 'adbr', 'adfl',\n",
    "       'bed1', 'bed2', 'bfo1', 'bfo2', 'bso1', 'bso2', 'bso3', 'ced1', 'cfo1',\n",
    "       'cso1']]\n",
    "test_df = cats_df[1000000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d9a34da-dda4-4211-b96b-a00900dc16a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class tkgngc_data_processing:\n",
    "    def __init__(self, data, device, num_timestamps=20):\n",
    "        self.data = data\n",
    "        self.ordered_column_names = self.data\n",
    "        self.train_list = self.data.values.tolist()\n",
    "        self.time_series_tensor = torch.tensor(self.train_list,dtype=torch.float32)\n",
    "        # Entity and Relation indices\n",
    "        self.entity_indices = torch.arange(len(self.train_list), dtype=torch.long)\n",
    "        self.relation_indices = torch.tensor(\n",
    "            [0 if i % 2 == 0 else 1 for i in range(len(self.entity_indices))],dtype=torch.long\n",
    "        )\n",
    "\n",
    "        self.timestamps = entity_indices\n",
    "        # Timestamp binning\n",
    "        self.num_timestamps = num_timestamps\n",
    "        min_time, max_time = min(self.entity_indices),max(self.entity_indices)\n",
    "        bins = torch.linspace(min_time,max_time,self.num_timestamps+1)\n",
    "        self.timestamp_indices = torch.tensor(torch.bucketize(self.entity_indices,bins),dtype=torch.long) - 1\n",
    "        self.timestamp_indices = torch.clamp(self.timestamp_indices, min = 0, max= num_timestamps -1)\n",
    "\n",
    "        # Edge index\n",
    "\n",
    "        self.edge_index = torch.tensor(\n",
    "            [[i,i+1] for i in range(len(self.entity_indices) - 1)],dtype=torch.long).t()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4730bec7-19a3-43eb-93fb-0a8fa55c9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_column_names = train.columns\n",
    "train_list = train.values.tolist()\n",
    "time_series_tensor = torch.tensor(train_list,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81820252-f10c-40a9-b3ad-17aa1386996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_indices = torch.arange(len(train_list), dtype=torch.long)\n",
    "relation_indices = torch.tensor(\n",
    "    [0 if i % 2 == 0 else 1 for i in range(len(entity_indices))],dtype=torch.long\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f51d3b7-3163-4ecc-8e38-d91340e2bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = entity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9151d52-8e57-4df0-9918-36fbcbfac406",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timestamps = 20\n",
    "min_time, max_time = min(entity_indices),max(entity_indices)\n",
    "bins = torch.linspace(min_time,max_time,num_timestamps+1)\n",
    "timestamp_indices = torch.tensor(torch.bucketize(entity_indices,bins),dtype=torch.long) - 1\n",
    "timestamp_indices = torch.clamp(timestamp_indices, min = 0, max= num_timestamps -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6fc660a-9aa4-461c-90cf-672ff461c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(\n",
    "    [[i,i+1] for i in range(len(entity_indices) - 1)],dtype=torch.long).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd9dffaa-f3b2-4740-ab4f-43812b5916e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_tensor = time_series_tensor.to(device)\n",
    "entity_indices=entity_indices.to(device)\n",
    "relation_indices=relation_indices.to(device)\n",
    "timestamp_indices=timestamp_indices.to(device)\n",
    "edge_index=edge_index.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58235119-fc47-49f4-bb09-850a764ea4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_tkg = PretrainedTKGEmbeddingWithTimestamps(\n",
    "    num_entities=int(entity_indices.max().item()+1),\n",
    "    num_relations=int(relation_indices.max().item()+1),\n",
    "    embedding_dim=16,\n",
    "    num_timestamps=num_timestamps,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43e68a17-d6de-4394-bde4-16c7d5333c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quads = (\n",
    "    entity_indices[:-1],  # Head entities\n",
    "    relation_indices[:-1],  # Relations\n",
    "    entity_indices[1:],  # Tail entities (shifted example)\n",
    "    timestamp_indices[:-1],  # Timestamps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c27d548-5966-4736-aa01-24fb8474941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.7462270259857178\n",
      "Epoch 10, Loss: 3.0316848754882812\n",
      "Epoch 20, Loss: 2.4488525390625\n",
      "Epoch 30, Loss: 1.983251690864563\n",
      "Epoch 40, Loss: 1.6134852170944214\n",
      "Epoch 50, Loss: 1.3197829723358154\n",
      "Epoch 60, Loss: 1.0855704545974731\n",
      "Epoch 70, Loss: 0.8976711630821228\n",
      "Epoch 80, Loss: 0.7459503412246704\n",
      "Epoch 90, Loss: 0.6226732134819031\n",
      "Epoch 100, Loss: 0.521932065486908\n",
      "Epoch 110, Loss: 0.43918588757514954\n",
      "Epoch 120, Loss: 0.3709111213684082\n",
      "Epoch 130, Loss: 0.3143463730812073\n",
      "Epoch 140, Loss: 0.26730769872665405\n",
      "Epoch 150, Loss: 0.22805573046207428\n",
      "Epoch 160, Loss: 0.19519668817520142\n",
      "Epoch 170, Loss: 0.16760790348052979\n",
      "Epoch 180, Loss: 0.14438001811504364\n",
      "Epoch 190, Loss: 0.12477302551269531\n",
      "Epoch 200, Loss: 0.10818178951740265\n",
      "Epoch 210, Loss: 0.09410931915044785\n",
      "Epoch 220, Loss: 0.08214595913887024\n",
      "Epoch 230, Loss: 0.0719527080655098\n",
      "Epoch 240, Loss: 0.06324820965528488\n",
      "Epoch 250, Loss: 0.05579833686351776\n",
      "Epoch 260, Loss: 0.049407705664634705\n",
      "Epoch 270, Loss: 0.04391295090317726\n",
      "Epoch 280, Loss: 0.039177168160676956\n",
      "Epoch 290, Loss: 0.03508543595671654\n",
      "Epoch 300, Loss: 0.0315411314368248\n",
      "Epoch 310, Loss: 0.028462857007980347\n",
      "Epoch 320, Loss: 0.025781983509659767\n",
      "Epoch 330, Loss: 0.023440539836883545\n",
      "Epoch 340, Loss: 0.021389514207839966\n",
      "Epoch 350, Loss: 0.019587412476539612\n",
      "Epoch 360, Loss: 0.017999062314629555\n",
      "Epoch 370, Loss: 0.016594624146819115\n",
      "Epoch 380, Loss: 0.015348738059401512\n",
      "Epoch 390, Loss: 0.014239837415516376\n",
      "Epoch 400, Loss: 0.013249562121927738\n",
      "Epoch 410, Loss: 0.01236224640160799\n",
      "Epoch 420, Loss: 0.011564522050321102\n",
      "Epoch 430, Loss: 0.010844954289495945\n",
      "Epoch 440, Loss: 0.010193752124905586\n",
      "Epoch 450, Loss: 0.009602521546185017\n",
      "Epoch 460, Loss: 0.009064044803380966\n",
      "Epoch 470, Loss: 0.008572112768888474\n",
      "Epoch 480, Loss: 0.0081213703379035\n",
      "Epoch 490, Loss: 0.00770718976855278\n"
     ]
    }
   ],
   "source": [
    "pretrained_tkg.pretrain(quads, learning_rate=0.01, epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aebeda2b-5ee2-4a4b-878b-5731781f2a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #: 0\n",
      "Iteration #: 5\n",
      "Iteration #: 10\n",
      "Iteration #: 15\n",
      "Iteration #: 20\n",
      "Iteration #: 25\n",
      "Iteration #: 30\n",
      "Iteration #: 35\n",
      "Iteration #: 40\n",
      "Iteration #: 45\n",
      "Iteration #: 50\n",
      "Iteration #: 55\n",
      "Iteration #: 60\n",
      "Iteration #: 65\n",
      "Iteration #: 70\n",
      "Iteration #: 75\n",
      "Iteration #: 80\n",
      "Iteration #: 85\n",
      "Iteration #: 90\n",
      "Iteration #: 95\n",
      "Iteration #: 100\n",
      "Iteration #: 105\n",
      "Iteration #: 110\n",
      "Iteration #: 115\n",
      "Iteration #: 120\n",
      "Iteration #: 125\n",
      "Iteration #: 130\n",
      "Iteration #: 135\n",
      "Iteration #: 140\n",
      "Iteration #: 145\n",
      "Iteration #: 150\n",
      "Iteration #: 155\n",
      "Iteration #: 160\n",
      "Iteration #: 165\n",
      "Iteration #: 170\n",
      "Iteration #: 175\n",
      "Iteration #: 180\n",
      "Iteration #: 185\n",
      "Iteration #: 190\n",
      "Iteration #: 195\n"
     ]
    }
   ],
   "source": [
    "new_metadata = []\n",
    "\n",
    "for i, row in enumerate(cats_rows_list):\n",
    "    potential_causes = metadata['root_cause'].unique().to_list()\n",
    "\n",
    "    start_time = datetime.strptime(row['start_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    end_time = datetime.strptime(row['end_time'],\"%Y-%m-%d %H:%M:%S\")\n",
    "    anomaly = eval(row['affected'])[0]\n",
    "    root_cause = row['root_cause']\n",
    "    potential_causes.append(anomaly)\n",
    "    mod_df = test_df[(test_df['timestamp']>= start_time) & (test_df['timestamp']<= end_time)]\n",
    "    test = mod_df[['aimp', 'amud', 'arnd', 'asin1', 'asin2', 'adbr', 'adfl',\n",
    "       'bed1', 'bed2', 'bfo1', 'bfo2', 'bso1', 'bso2', 'bso3', 'ced1', 'cfo1',\n",
    "       'cso1']]\n",
    "    test_data = tkgngc_data_processing(data=test, device=device, num_timestamps=20)\n",
    "    # Instantiate the full model\n",
    "\n",
    "    entity_emb, relation_emb, _, timestamp_emb = pretrained_tkg(\n",
    "    test_data.entity_indices, test_data.relation_indices, test_data.entity_indices, test_data.timestamp_indices\n",
    ")\n",
    "\n",
    "    model = NGCWithPretrainedTKGAndTimestamps(\n",
    "        pretrained_tkg=pretrained_tkg,\n",
    "        input_dim=test_data.time_series_tensor.shape[1],\n",
    "        hidden_dim=64,\n",
    "        output_dim=test_data.time_series_tensor.shape[1],\n",
    "        confounder_latent_dim=17,\n",
    "        entity_indices=test_data.entity_indices,\n",
    "        relation_indices=test_data.relation_indices,\n",
    "        time_series_data=test_data.time_series_tensor,\n",
    "        timestamp_indices=test_data.timestamp_indices,\n",
    "        edge_index=test_data.edge_index,\n",
    "        use_sliding_window=False,\n",
    "        window_size=10,\n",
    "        step_size=2,\n",
    "        regularization_type=\"l1\",\n",
    "        regularization_strength=0.01,\n",
    "    )\n",
    "    \"\"\"for j in range(50):\n",
    "        # Forward pass with the processed data\n",
    "         z, mean, log_var, x_reconstructed = model(\n",
    "            entity_indices=test_data.entity_indices,\n",
    "            relation_indices=test_data.relation_indices,\n",
    "            time_series_data=test_data.time_series_tensor,\n",
    "            edge_index=test_data.edge_index,\n",
    "            timestamp_indices=test_data.timestamp_indices,\n",
    "        )\"\"\"\n",
    "    model.train()\n",
    "    score_df = pd.DataFrame(np.mean(np.abs(model.z.detach().numpy()),axis=0),\n",
    "             index=ordered_column_names,columns=['scores']).sort_values(by=['scores'], ascending=False)\n",
    "    remvove_list = list(set(score_df.index).difference(set(potential_causes)))\n",
    "    score_df = score_df.drop(remvove_list)\n",
    "\n",
    "    potential_cause1 = score_df['scores'].index[0]\n",
    "    potential_cause2 = score_df['scores'].index[1]\n",
    "    potential_cause3 = score_df['scores'].index[2]\n",
    "    if root_cause == potential_cause1:\n",
    "        row['cause_1'] = 1\n",
    "    if root_cause == potential_cause2:\n",
    "        row['cause_2'] = 1\n",
    "    if root_cause == potential_cause3:\n",
    "        row['cause_3'] = 1\n",
    "    new_metadata.append(row)\n",
    "\n",
    "    if i%5 == 0:\n",
    "        print('Iteration #: ' + str(i))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d678050c-c596-48cf-b0a4-1a958a904265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pl.DataFrame(new_metadata)\n",
    "agg_stats = stats.select(pl.sum(\"cause_1\", \"cause_2\",'cause_3'))\n",
    "agg_stats.select(pl.sum_horizontal(pl.all())).item()/stats.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806d384-dad2-4f34-9045-8230dbfd2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee87754e-48c9-4b0d-9efe-f34f2ae08dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from tkgngc.utils import vae_loss\n",
    "\n",
    "def create_lagged_data(time_series_data, num_lags):\n",
    "    \"\"\"\n",
    "    Create lagged versions of time-series data.\n",
    "    \n",
    "    Args:\n",
    "        time_series_data (torch.Tensor): Time-series data of shape [batch_size, num_nodes, input_dim, time_steps].\n",
    "        num_lags (int): Number of lags to include.\n",
    "        \n",
    "    Returns:\n",
    "        lagged_data (torch.Tensor): Lagged data of shape [batch_size, num_nodes, input_dim * num_lags, time_steps - num_lags].\n",
    "        current_data (torch.Tensor): Current time steps of shape [batch_size, num_nodes, input_dim, time_steps - num_lags].\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, input_dim, time_steps = time_series_data.shape\n",
    "\n",
    "    # Prepare lagged data\n",
    "    lagged_data = []\n",
    "    for lag in range(1, num_lags + 1):\n",
    "        lagged_data.append(time_series_data[..., :-lag])  # Remove the last `lag` steps\n",
    "\n",
    "    # Stack lagged data along the feature dimension\n",
    "    lagged_data = torch.cat(lagged_data, dim=2)  # Shape: [batch_size, num_nodes, input_dim * num_lags, time_steps - num_lags]\n",
    "\n",
    "    # Current data excludes the first `num_lags` time steps\n",
    "    current_data = time_series_data[..., num_lags:]  # Shape: [batch_size, num_nodes, input_dim, time_steps - num_lags]\n",
    "\n",
    "    return lagged_data, current_data\n",
    "\n",
    "class Sampling(nn.Module):\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        # get the shape of the tensor for the mean and log variance\n",
    "        batch, dim = z_mean.shape\n",
    "        # generate a normal random tensor (epsilon) with the same shape as z_mean\n",
    "        # this tensor will be used for reparameterization trick\n",
    "        epsilon = Normal(0, 1).sample((batch, dim)).to(z_mean.device)\n",
    "        # apply the reparameterization trick to generate the samples in the\n",
    "        # latent space\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-Attention Module.\"\"\"\n",
    "    def __init__(self, dim_query, dim_key_value, dim_out, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_proj = nn.Linear(dim_query, dim_out)\n",
    "        self.key_proj = nn.Linear(dim_key_value, dim_out)\n",
    "        self.value_proj = nn.Linear(dim_key_value, dim_out)\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_out // num_heads\n",
    "\n",
    "        assert dim_out % num_heads == 0, \"Output dimension must be divisible by the number of heads.\"\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Linear projections\n",
    "        Q = self.query_proj(query).view(query.size(0), -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
    "        K = self.key_proj(key).view(key.size(0), -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
    "        V = self.value_proj(value).view(value.size(0), -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_weights = torch.matmul(Q, K.transpose(-1, -2)) / (self.dim_head ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Attention output\n",
    "        attn_output = torch.matmul(attn_weights, V).transpose(1, 2).reshape(query.size(0), -1, self.num_heads * self.dim_head)\n",
    "        return attn_output\n",
    "\n",
    "class GraphAttentionGC(nn.Module):\n",
    "    \"\"\"Graph Attention Network for Granger Causality.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=4):\n",
    "        super(GraphAttentionGC, self).__init__()\n",
    "        self.gat1 = GATv2Conv(input_dim, hidden_dim, heads=num_heads)\n",
    "        self.gat2 = GATv2Conv(hidden_dim * num_heads, output_dim, heads=1, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GrangerCausality(nn.Module):\n",
    "    \"\"\"Granger Causality Model with GAT and Cross-Attention in the Decoder.\"\"\"\n",
    "    def __init__(self, pretrained_tkg, input_dim, hidden_dim, latent_dim, num_heads, num_nodes, num_lags):\n",
    "        super(GrangerCausality, self).__init__()\n",
    "\n",
    "        self.pretrained_tkg = pretrained_tkg\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_lags = num_lags\n",
    "\n",
    "        # Graph Attention Network in Encoder\n",
    "        self.gat_encoder = GATv2Conv(input_dim + pretrained_tkg.entity_embedding.embedding_dim, hidden_dim, heads=num_heads)\n",
    "\n",
    "        # Cross-Attention in Encoder\n",
    "        self.cross_attention_encoder = CrossAttention(\n",
    "            dim_query=hidden_dim,\n",
    "            dim_key_value=(input_dim + pretrained_tkg.entity_embedding.embedding_dim) * num_lags,\n",
    "            dim_out=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        # Variational Components\n",
    "        self.encoder_fc = nn.Linear(hidden_dim, latent_dim * 2)  # Mean and log variance\n",
    "\n",
    "        # Cross-Attention in Decoder\n",
    "        self.cross_attention_decoder = CrossAttention(\n",
    "            dim_query=latent_dim,\n",
    "            dim_key_value=(input_dim + pretrained_tkg.entity_embedding.embedding_dim) * num_lags,\n",
    "            dim_out=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        # Graph Attention in Decoder\n",
    "        self.gat_decoder = GATv2Conv(hidden_dim, hidden_dim, heads=num_heads)\n",
    "\n",
    "        # Fully Connected Layer to Reconstruct Input\n",
    "        self.decoder_fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        # Adjacency Matrix for Causal Graph\n",
    "        self.adjacency_matrix = nn.Parameter(torch.randn(num_nodes, num_nodes, num_lags))\n",
    "\n",
    "    def forward(self, entity_indices, relation_indices, timestamp_indices, time_series_data, edge_index, lagged_data):\n",
    "        \"\"\"\n",
    "        Forward pass for Granger causality detection with GAT and cross-attention in the decoder.\n",
    "        Args:\n",
    "            entity_indices (torch.Tensor): Indices of entities.\n",
    "            relation_indices (torch.Tensor): Indices of relations.\n",
    "            timestamp_indices (torch.Tensor): Indices of timestamps.\n",
    "            time_series_data (torch.Tensor): Original time-series data.\n",
    "            edge_index (torch.Tensor): Graph edges.\n",
    "            lagged_data (torch.Tensor): Lagged time-series data of shape [batch, num_nodes, lagged_features].\n",
    "        \"\"\"\n",
    "        # Pretrained TKG embeddings\n",
    "        entity_emb, relation_emb, _, timestamp_emb = self.pretrained_tkg(\n",
    "            entity_indices, relation_indices, entity_indices, timestamp_indices\n",
    "        )\n",
    "        entity_emb = entity_emb.unsqueeze(-2).expand(-1, -1, time_series_data.size(-1))  # Match time steps\n",
    "\n",
    "        # Concatenate TKG embeddings with time-series data\n",
    "        enriched_features = torch.cat([time_series_data, entity_emb], dim=-1)  # [batch, num_nodes, input_dim + embedding_dim]\n",
    "\n",
    "        # Encoder: Graph Attention\n",
    "        x = self.gat_encoder(enriched_features, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Encoder: Cross-Attention\n",
    "        x, _ = self.cross_attention_encoder(x, lagged_data, lagged_data)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Latent Space\n",
    "        q_params = self.encoder_fc(x)\n",
    "        mean, log_var = torch.chunk(q_params, 2, dim=-1)\n",
    "\n",
    "        # Reparameterization Trick\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        z = mean + std * torch.randn_like(std)\n",
    "\n",
    "        # Decoder: Cross-Attention\n",
    "        x, _ = self.cross_attention_decoder(z, lagged_data, lagged_data)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Decoder: Graph Attention\n",
    "        x = self.gat_decoder(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Decoder: Fully Connected Reconstruction\n",
    "        x_reconstructed = self.decoder_fc(x)\n",
    "\n",
    "        # Learned Adjacency Matrix\n",
    "        adj = torch.sigmoid(self.adjacency_matrix)  # Values in [0, 1]\n",
    "\n",
    "        return z, mean, log_var, x_reconstructed, adj\n",
    "\n",
    "    def loss_function(self, x_reconstructed, time_series_data, mean, log_var, adj, sparsity_weight=0.01, beta=1.0):\n",
    "        \"\"\"\n",
    "        Loss function for Granger causality with GAT and cross-attention in the decoder.\n",
    "        Args:\n",
    "            x_reconstructed (torch.Tensor): Reconstructed features.\n",
    "            time_series_data (torch.Tensor): Original input features.\n",
    "            mean (torch.Tensor): Mean of latent distribution.\n",
    "            log_var (torch.Tensor): Log variance of latent distribution.\n",
    "            adj (torch.Tensor): Learned adjacency matrix.\n",
    "            sparsity_weight (float): Weight for sparsity regularization.\n",
    "            beta (float): Weight for KL divergence.\n",
    "        \"\"\"\n",
    "        # Reconstruction Loss\n",
    "        recon_loss = F.mse_loss(x_reconstructed, time_series_data, reduction='sum')\n",
    "\n",
    "        # KL Divergence\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "        # Sparsity Regularization\n",
    "        sparsity_loss = sparsity_weight * torch.sum(torch.abs(adj))\n",
    "\n",
    "        # Total Loss\n",
    "        return recon_loss + beta * kl_divergence + sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "197150e5-23ef-4cdb-9e32-79949f1aec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GrangerCausality(pretrained_tkg ,input_dim=16, hidden_dim=32, latent_dim=16, num_heads=4, num_nodes=10, num_lags=5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5601ff39-1c1a-401b-a52c-8320b74ad301",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tkgngc_data_processing(data=test, device=device, num_timestamps=20)\n",
    "# Instantiate the full model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fe2d972-1fe1-4a99-979f-0427f40578a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_data(time_series_data, num_lags):\n",
    "    \"\"\"\n",
    "    Create lagged versions of time-series data.\n",
    "    \n",
    "    Args:\n",
    "        time_series_data (torch.Tensor): Time-series data of shape [batch_size, num_nodes, input_dim, time_steps].\n",
    "        num_lags (int): Number of lags to include.\n",
    "        \n",
    "    Returns:\n",
    "        lagged_data (torch.Tensor): Lagged data of shape [batch_size, num_nodes, input_dim * num_lags, time_steps - num_lags].\n",
    "        current_data (torch.Tensor): Current time steps of shape [batch_size, num_nodes, input_dim, time_steps - num_lags].\n",
    "    \"\"\"\n",
    "    batch_size, num_nodes, input_dim, time_steps = time_series_data.shape\n",
    "\n",
    "    # Prepare lagged data\n",
    "    lagged_data = []\n",
    "    for lag in range(1, num_lags + 1):\n",
    "        lagged_data.append(time_series_data[..., :-lag])  # Remove the last `lag` steps\n",
    "\n",
    "    # Stack lagged data along the feature dimension\n",
    "    lagged_data = torch.cat(lagged_data, dim=2)  # Shape: [batch_size, num_nodes, input_dim * num_lags, time_steps - num_lags]\n",
    "\n",
    "    # Current data excludes the first `num_lags` time steps\n",
    "    current_data = time_series_data[..., num_lags:]  # Shape: [batch_size, num_nodes, input_dim, time_steps - num_lags]\n",
    "\n",
    "    return lagged_data, current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a00cef3f-2f68-4647-bf1a-5df860ba53f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m time_series_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mtime_series_tensor\n\u001b[0;32m----> 2\u001b[0m lagged_data, original \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_lagged_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_series_data\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 13\u001b[0m, in \u001b[0;36mcreate_lagged_data\u001b[0;34m(time_series_data, num_lags)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_lagged_data\u001b[39m(time_series_data, num_lags):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Create lagged versions of time-series data.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        current_data (torch.Tensor): Current time steps of shape [batch_size, num_nodes, input_dim, time_steps - num_lags].\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     batch_size, num_nodes, input_dim, time_steps \u001b[38;5;241m=\u001b[39m time_series_data\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Prepare lagged data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     lagged_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "time_series_data = test_data.time_series_tensor\n",
    "lagged_data, original = create_lagged_data(time_series_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd136ee-b259-47d8-85e6-0365a948c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example data\n",
    "time_series_data = torch.randn(32, 10, 16)  # Batch of 32 graphs with 10 nodes, 16 features each\n",
    "lagged_data = torch.randn(32, 10, 80)  # Lagged data: num_lags * input_dim = 5 * 16\n",
    "edge_index = torch.randint(0, 10, (2, 20))  # Random graph edges\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    z, mean, log_var, x_reconstructed, adj, attn_weights = model(time_series_data, edge_index, lagged_data)\n",
    "    loss = model.loss_function(x_reconstructed, time_series_data, mean, log_var, adj)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rca",
   "language": "python",
   "name": "rca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
